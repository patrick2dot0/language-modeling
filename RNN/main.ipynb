{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/god/.local/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from ptb import PTB\n",
    "from model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU')\n",
    "else:\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penn TreeBank (PTB) dataset\n",
    "data_path = '../data'\n",
    "max_len = 96\n",
    "splits = ['train', 'valid', 'test']\n",
    "datasets = {split: PTB(root=data_path, split=split) for split in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "batch_size = 20 #32\n",
    "dataloaders = {split: DataLoader(datasets[split],\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=split=='train',\n",
    "                                 num_workers=cpu_count(),\n",
    "                                 pin_memory=torch.cuda.is_available())\n",
    "                                 for split in splits}\n",
    "symbols = datasets['train'].symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(10002, 300, padding_idx=0)\n",
      "  (rnn): LSTM(300, 450, batch_first=True)\n",
      "  (output): Linear(in_features=450, out_features=10002, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# RNN model\n",
    "embedding_size = 300\n",
    "hidden_size = 450 #256\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = RNN(vocab_size=datasets['train'].vocab_size,\n",
    "            embed_size=embedding_size,\n",
    "            time_step=max_len,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            bos_idx=symbols['<bos>'],\n",
    "            eos_idx=symbols['<eos>'],\n",
    "            pad_idx=symbols['<pad>'])\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "for p in model.parameters():\n",
    "    p.data.uniform_(-0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save model\n",
    "save_path = 'model'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/god/.local/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# objective function\n",
    "learning_rate = 40#0.001\n",
    "criterion = nn.NLLLoss(size_average=False, ignore_index=symbols['<pad>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) #Adam\n",
    "\n",
    "# negative log likelihood\n",
    "def NLL(logp, target, length):\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(-1))\n",
    "    return criterion(logp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setting\n",
    "epoch = 20\n",
    "print_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/2104, NLL-Loss 205.1002, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 163.0776, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 164.3962, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 153.9433, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 156.9258, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 123.7360, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 127.2849, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 152.4349, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 114.6360, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 107.0449, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 123.7056, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 132.6186, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 121.1500, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 143.3746, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 117.0036, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 110.7385, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 138.4691, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 124.3361, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 112.4763, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 123.8357, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 123.9144, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 111.3361, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 123.7838, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 129.2278, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 112.9260, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 121.6994, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 130.2174, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 112.4879, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 120.2633, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 108.4815, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 121.9073, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 111.9464, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 122.3316, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 95.4610, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 138.2540, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 123.8934, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 115.1200, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 114.8275, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 131.4417, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 103.6548, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 131.3651, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 100.3912, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 107.2371, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 96.3402, \n",
      "TRAIN Epoch 00/20, NLL 126.8265, PPL 408.0972\n",
      "VALID Epoch 00/20, NLL 113.6378, PPL 230.5660\n",
      "TEST Epoch 00/20, NLL 113.2196, PPL 224.2565\n",
      "Model saved at model/E00.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 144.1930, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 94.3833, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 102.6024, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 137.2696, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 110.6151, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 117.0060, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 126.4075, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 103.4784, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 121.6052, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 128.5651, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 122.2322, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 108.6272, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 106.8547, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 110.2740, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 139.5645, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 108.0725, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 111.6797, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 133.3538, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 103.0732, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 113.4504, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 105.2104, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 124.1750, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 122.5220, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 126.8004, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 119.2383, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 118.3241, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 119.8720, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 129.6528, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 124.0615, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 89.6827, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 142.2051, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 114.9891, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 100.3366, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 112.7425, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 102.2781, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 114.2023, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 109.2895, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 83.9449, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 112.9067, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 102.8615, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 91.9694, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 109.6006, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 102.5522, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 129.6570, \n",
      "TRAIN Epoch 01/20, NLL 114.7508, PPL 230.2408\n",
      "VALID Epoch 01/20, NLL 109.3019, PPL 187.3448\n",
      "TEST Epoch 01/20, NLL 108.7467, PPL 181.0819\n",
      "Model saved at model/E01.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 114.8029, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 117.5888, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 114.8860, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 120.1345, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 115.2557, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 110.5118, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 124.7229, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 102.7230, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 117.2664, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 102.7844, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 97.0035, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 114.2397, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 96.1116, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 103.0961, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 89.9315, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 124.8409, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 108.7691, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 111.9469, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 100.7082, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 101.5750, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 101.3904, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 87.2868, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 109.9402, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 106.9838, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 96.7156, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 92.2555, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 85.6044, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 79.3051, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 93.3868, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 84.1473, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 103.2932, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 107.1517, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 107.3080, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 98.0929, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 112.7212, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 101.5177, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 129.0893, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 107.4358, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 93.4987, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 117.4348, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 94.6491, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 87.5123, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 98.8645, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 82.7158, \n",
      "TRAIN Epoch 02/20, NLL 108.6870, PPL 172.7252\n",
      "VALID Epoch 02/20, NLL 105.8080, PPL 158.4874\n",
      "TEST Epoch 02/20, NLL 105.2209, PPL 152.9928\n",
      "Model saved at model/E02.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 94.2557, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 114.8044, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 108.6872, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 118.9517, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 81.4617, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 114.4533, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 130.1481, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 101.8873, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 101.0514, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 115.7315, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 91.8717, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 114.9730, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 90.8364, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 92.4797, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 106.7429, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 108.7358, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 128.6033, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 107.7679, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 99.0693, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 111.1009, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 114.0978, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 111.4497, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 99.9198, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 113.5985, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 104.3204, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 82.7277, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 122.9596, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 107.0288, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 92.3724, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 105.1587, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 102.0421, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 108.1644, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 110.7099, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 97.2461, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 139.8850, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 89.2418, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 116.1664, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 94.3818, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 85.4937, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 105.4127, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 122.3534, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 107.0786, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 92.2789, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 73.7214, \n",
      "TRAIN Epoch 03/20, NLL 106.1639, PPL 153.2562\n",
      "VALID Epoch 03/20, NLL 104.4421, PPL 148.4548\n",
      "TEST Epoch 03/20, NLL 103.8134, PPL 143.0368\n",
      "Model saved at model/E03.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/2104, NLL-Loss 101.1075, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 111.0943, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 81.1327, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 110.8152, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 111.5830, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 124.3795, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 116.5560, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 96.0956, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 94.4773, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 88.3573, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 99.0879, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 108.8426, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 109.3814, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 100.9349, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 80.0676, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 105.0107, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 121.5751, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 110.7120, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 83.6839, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 97.8394, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 115.1070, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 82.5005, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 83.8736, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 102.9494, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 89.6679, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 118.2394, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 109.2226, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 96.9235, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 108.7225, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 101.5398, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 93.1236, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 126.0372, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 107.7326, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 113.3736, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 109.1685, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 79.8141, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 105.9954, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 94.1734, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 94.8757, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 81.8064, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 88.5277, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 106.6173, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 97.8566, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 78.5988, \n",
      "TRAIN Epoch 04/20, NLL 103.2333, PPL 133.3796\n",
      "VALID Epoch 04/20, NLL 103.2030, PPL 139.9045\n",
      "TEST Epoch 04/20, NLL 102.5512, PPL 134.6606\n",
      "Model saved at model/E04.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 91.0005, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 106.3568, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 104.5109, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 100.9989, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 102.5418, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 101.1568, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 109.6967, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 85.0224, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 107.9186, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 89.8246, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 107.5504, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 108.7669, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 108.7907, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 90.3892, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 113.5069, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 101.9785, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 110.3418, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 108.8986, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 97.6213, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 115.5169, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 93.9039, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 97.6244, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 102.8926, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 109.1127, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 92.8326, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 97.7201, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 88.3626, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 113.7094, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 96.0520, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 90.2485, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 115.6500, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 105.6395, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 93.6381, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 87.2690, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 106.1508, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 103.5219, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 99.8924, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 87.9323, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 92.2962, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 105.0829, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 87.4925, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 96.9629, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 129.0578, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 114.2735, \n",
      "TRAIN Epoch 05/20, NLL 102.0030, PPL 125.8244\n",
      "VALID Epoch 05/20, NLL 102.7174, PPL 136.6894\n",
      "TEST Epoch 05/20, NLL 102.0559, PPL 131.5095\n",
      "Model saved at model/E05.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 105.2739, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 106.2971, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 96.7107, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 90.9899, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 91.7237, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 99.8916, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 111.3906, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 109.7315, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 112.3142, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 122.4792, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 99.1903, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 94.9225, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 110.4710, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 98.0567, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 98.0413, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 102.3964, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 106.1102, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 98.5572, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 102.6541, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 109.3089, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 101.4802, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 115.5557, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 99.2970, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 100.6075, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 118.2734, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 122.1392, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 99.2707, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 102.4969, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 93.8845, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 99.8676, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 112.7902, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 105.8375, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 98.0991, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 124.3140, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 104.4714, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 90.6843, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 91.5016, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 91.9420, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 119.6840, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 109.8771, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 116.4185, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 90.3372, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 100.3226, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 76.8975, \n",
      "TRAIN Epoch 06/20, NLL 100.4279, PPL 116.7725\n",
      "VALID Epoch 06/20, NLL 102.1826, PPL 133.2339\n",
      "TEST Epoch 06/20, NLL 101.4859, PPL 127.9740\n",
      "Model saved at model/E06.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 102.4207, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 107.4203, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 106.6406, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 83.9642, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 108.8189, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 91.3431, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 100.7125, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 119.0687, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 128.7892, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 83.6546, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 99.6505, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 102.6573, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 103.1590, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 93.1438, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 93.8275, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 100.7514, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 113.1692, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 98.5745, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 87.9269, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 118.3232, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 82.9617, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 83.4095, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 87.0050, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 105.9899, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 95.4924, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 91.1170, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 87.5753, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 89.1019, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 103.9600, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 109.3145, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 101.0663, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 86.4484, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 106.3825, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 91.0907, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 97.4435, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 97.2441, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 70.9675, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 103.9547, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 108.0028, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 100.9633, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 98.2920, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 104.7647, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 92.7507, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 108.1717, \n",
      "TRAIN Epoch 07/20, NLL 99.7292, PPL 112.9686\n",
      "VALID Epoch 07/20, NLL 101.9029, PPL 131.4615\n",
      "TEST Epoch 07/20, NLL 101.2054, PPL 126.2693\n",
      "Model saved at model/E07.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 104.1996, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0050/2104, NLL-Loss 108.0003, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 112.9936, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 90.6010, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 82.8587, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 91.3092, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 99.5268, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 97.0355, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 108.3455, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 95.5621, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 94.6084, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 106.0120, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 107.0856, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 99.3507, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 92.7055, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 97.0157, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 113.1460, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 97.7768, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 91.2451, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 75.6502, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 102.5477, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 88.5876, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 110.0151, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 112.9438, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 98.8157, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 95.6632, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 94.7677, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 100.6197, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 105.3199, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 110.8846, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 104.4556, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 104.0891, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 93.1859, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 102.4233, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 99.0845, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 87.4830, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 83.8532, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 80.6110, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 121.5588, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 89.9109, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 89.2403, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 92.5778, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 92.2507, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 91.6810, \n",
      "TRAIN Epoch 08/20, NLL 98.9075, PPL 108.6533\n",
      "VALID Epoch 08/20, NLL 101.7331, PPL 130.3975\n",
      "TEST Epoch 08/20, NLL 101.0293, PPL 125.2110\n",
      "Model saved at model/E08.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 114.5761, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 104.0699, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 109.6220, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 116.3851, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 102.5802, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 88.1962, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 82.6743, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 83.8229, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 88.2147, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 97.6306, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 90.2673, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 90.0701, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 97.6151, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 95.7496, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 109.6916, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 128.6568, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 82.2579, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 107.1524, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 84.2851, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 96.9357, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 103.2136, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 119.1356, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 97.1372, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 97.3067, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 105.9585, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 110.9459, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 90.5895, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 83.2836, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 92.8851, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 81.8692, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 99.5287, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 118.0775, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 88.7098, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 96.3492, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 100.9024, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 111.0757, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 81.4376, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 107.1265, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 95.3761, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 104.6457, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 104.4952, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 96.4918, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 95.1794, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 67.1085, \n",
      "TRAIN Epoch 09/20, NLL 98.5778, PPL 106.9685\n",
      "VALID Epoch 09/20, NLL 101.6154, PPL 129.6646\n",
      "TEST Epoch 09/20, NLL 100.9206, PPL 124.5619\n",
      "Model saved at model/E09.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 90.4786, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 86.1689, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 100.5117, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 97.4995, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 112.2395, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 98.8899, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 98.5575, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 86.8845, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 95.4877, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 94.6129, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 98.4990, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 100.0997, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 114.3300, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 108.6903, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 121.0881, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 99.8600, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 115.9243, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 88.0245, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 106.5371, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 88.9782, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 94.2480, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 105.0192, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 108.1570, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 94.9921, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 118.7717, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 103.7946, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 86.5320, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 90.7007, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 97.5677, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 98.0274, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 87.6720, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 105.2653, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 96.6359, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 84.8572, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 94.7467, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 119.1351, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 91.7202, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 94.0186, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 103.6188, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 89.2386, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 107.7507, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 96.0450, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 102.3646, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 75.1420, \n",
      "TRAIN Epoch 10/20, NLL 98.1391, PPL 104.7671\n",
      "VALID Epoch 10/20, NLL 101.5450, PPL 129.2282\n",
      "TEST Epoch 10/20, NLL 100.8188, PPL 123.9568\n",
      "Model saved at model/E10.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 99.5419, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 101.9476, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 91.9848, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 97.4931, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 92.8746, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 103.6329, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 90.5422, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 112.3543, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 87.7225, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 71.5009, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 99.1443, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 98.3541, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 103.5925, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 98.2852, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 114.1085, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 86.5036, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 94.7845, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 119.9592, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 108.7395, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 94.7521, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 100.2162, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 83.7074, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 91.4886, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 86.8301, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 97.1383, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 116.0509, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 94.0151, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 110.5554, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 92.5668, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 115.8283, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 105.1994, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 74.1575, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 100.6919, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 105.3019, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 86.8619, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 77.7741, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 105.7646, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 92.3985, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 105.0623, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 79.6528, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 100.4050, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 104.1165, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 102.4485, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 97.4178, \n",
      "TRAIN Epoch 11/20, NLL 97.9619, PPL 103.8908\n",
      "VALID Epoch 11/20, NLL 101.4644, PPL 128.7306\n",
      "TEST Epoch 11/20, NLL 100.7506, PPL 123.5537\n",
      "Model saved at model/E11.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 82.8225, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 98.6005, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 106.6503, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0150/2104, NLL-Loss 92.9893, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 85.0956, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 97.4712, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 111.3325, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 93.2810, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 106.0800, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 84.1591, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 96.4337, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 84.3714, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 87.5873, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 86.7672, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 112.2869, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 106.6448, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 98.3761, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 111.4753, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 88.5718, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 98.3462, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 88.7632, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 106.2341, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 104.3982, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 106.6657, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 83.2334, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 90.9297, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 96.0898, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 90.1887, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 99.7223, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 78.8607, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 102.8914, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 97.5757, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 82.5250, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 110.1115, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 100.1084, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 93.7671, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 81.1866, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 101.9678, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 100.2505, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 93.3452, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 107.0936, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 89.0428, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 96.0877, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 121.4721, \n",
      "TRAIN Epoch 12/20, NLL 97.7621, PPL 102.9115\n",
      "VALID Epoch 12/20, NLL 101.4333, PPL 128.5388\n",
      "TEST Epoch 12/20, NLL 100.6987, PPL 123.2473\n",
      "Model saved at model/E12.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 102.0373, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 113.0005, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 115.2601, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 95.4151, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 89.6055, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 108.9536, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 100.0073, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 103.8886, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 89.3561, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 93.1542, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 99.2094, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 98.6537, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 86.3718, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 97.8899, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 69.5611, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 74.8755, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 95.8791, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 97.5984, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 111.5374, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 96.3430, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 118.4052, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 97.0413, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 95.1150, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 89.4104, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 91.5995, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 89.3326, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 88.1905, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 88.1040, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 101.4767, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 82.4958, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 106.7640, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 90.8732, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 91.1526, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 113.2703, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 95.8506, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 127.2513, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 80.0369, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 90.6283, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 112.9719, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 99.4402, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 126.1952, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 87.6554, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 96.8124, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 114.5549, \n",
      "TRAIN Epoch 13/20, NLL 97.6398, PPL 102.3165\n",
      "VALID Epoch 13/20, NLL 101.4217, PPL 128.4676\n",
      "TEST Epoch 13/20, NLL 100.6951, PPL 123.2260\n",
      "Model saved at model/E13.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 95.8844, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 80.2899, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 96.8786, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 96.0926, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 99.8321, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 77.0987, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 89.6641, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 99.7696, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 101.6247, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 95.4979, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 93.1093, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 97.0658, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 103.8775, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 84.3910, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 100.3298, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 85.5530, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 98.1735, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 106.8458, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 99.0015, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 92.4400, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 106.3727, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 87.1467, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 110.0594, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 96.6434, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 115.3929, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 114.7612, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 103.3273, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 103.0543, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 93.5436, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 94.8969, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 103.9816, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 95.7133, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 99.7596, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 93.8948, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 77.1290, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 103.2792, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 99.8007, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 86.3437, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 105.0727, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 86.6557, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 90.2221, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 101.6268, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 97.5011, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 121.9220, \n",
      "TRAIN Epoch 14/20, NLL 97.5152, PPL 101.7139\n",
      "VALID Epoch 14/20, NLL 101.3947, PPL 128.3015\n",
      "TEST Epoch 14/20, NLL 100.6725, PPL 123.0931\n",
      "Model saved at model/E14.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 92.7225, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 91.2993, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 96.6528, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 118.5084, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 99.9693, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 101.8809, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 94.9406, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 82.1385, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 91.7305, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 95.6950, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 98.0977, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 102.2644, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 89.7759, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 105.2539, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 87.3905, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 80.6932, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 98.6982, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 90.1257, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 84.5071, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 81.3029, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 86.1962, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 96.9932, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 88.5501, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 107.8951, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 123.8511, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 91.3358, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 82.9019, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 103.0102, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 78.3372, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 102.0385, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 102.2044, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 102.1754, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 103.2271, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 76.5031, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 103.1500, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 92.1413, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 89.4317, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 111.7180, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 96.4395, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 95.7879, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 98.3174, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 112.5541, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 104.0901, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 84.3486, \n",
      "TRAIN Epoch 15/20, NLL 97.5278, PPL 101.7750\n",
      "VALID Epoch 15/20, NLL 101.3826, PPL 128.2274\n",
      "TEST Epoch 15/20, NLL 100.6667, PPL 123.0592\n",
      "Model saved at model/E15.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 93.8320, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 110.1263, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 98.6643, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 88.3823, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 88.9794, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0250/2104, NLL-Loss 94.4444, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 97.8744, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 99.6775, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 87.3081, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 88.5922, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 97.7767, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 101.6478, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 107.7234, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 105.3501, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 96.5990, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 110.4589, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 118.1229, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 89.0330, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 96.0839, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 134.1119, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 92.5848, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 113.3227, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 106.9671, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 105.2266, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 110.5178, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 100.5084, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 92.6189, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 103.4856, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 88.6809, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 93.3478, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 79.3620, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 92.1037, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 98.1673, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 80.0324, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 93.0909, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 99.5195, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 101.2303, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 99.0717, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 84.0836, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 93.4306, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 111.8226, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 99.9117, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 98.4378, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 71.8018, \n",
      "TRAIN Epoch 16/20, NLL 97.4339, PPL 101.3228\n",
      "VALID Epoch 16/20, NLL 101.3764, PPL 128.1894\n",
      "TEST Epoch 16/20, NLL 100.6540, PPL 122.9845\n",
      "Model saved at model/E16.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 103.5025, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 94.0592, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 104.0299, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 110.5980, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 103.6766, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 108.6544, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 97.0237, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 101.1459, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 100.0433, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 94.0517, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 95.0698, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 102.0617, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 101.0715, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 108.7203, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 108.9007, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 103.7824, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 76.3339, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 94.9495, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 81.7307, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 102.8682, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 83.1119, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 95.9942, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 102.4421, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 99.1432, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 91.7996, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 123.0557, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 65.6966, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 91.9874, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 123.5064, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 97.5254, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 138.8458, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 86.0155, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 108.9519, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 130.4055, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 138.8484, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 84.6034, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 85.1186, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 109.4561, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 80.4338, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 105.4272, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 102.7492, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 114.8819, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 86.3581, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 94.0940, \n",
      "TRAIN Epoch 17/20, NLL 97.4249, PPL 101.2795\n",
      "VALID Epoch 17/20, NLL 101.3600, PPL 128.0890\n",
      "TEST Epoch 17/20, NLL 100.6378, PPL 122.8892\n",
      "Model saved at model/E17.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 93.0876, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 85.3860, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 104.8596, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 102.4003, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 100.8877, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 94.8682, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 83.4217, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 83.3425, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 107.8443, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 97.0894, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 82.3435, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 101.9398, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 89.5226, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 86.7938, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 84.4322, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 104.3863, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 103.1523, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 101.2945, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 86.4728, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 97.7355, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 78.7067, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 125.7079, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 88.6012, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 86.6048, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 111.0202, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 98.2033, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 85.5789, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 104.8154, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 86.7670, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 114.0906, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 105.0478, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 85.0499, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 91.8249, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 78.4580, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 81.3125, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 101.2902, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 98.6445, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 125.8272, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 94.5453, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 97.9906, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 88.7607, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 101.0350, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 103.1936, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 102.9142, \n",
      "TRAIN Epoch 18/20, NLL 97.4088, PPL 101.2026\n",
      "VALID Epoch 18/20, NLL 101.3555, PPL 128.0612\n",
      "TEST Epoch 18/20, NLL 100.6350, PPL 122.8725\n",
      "Model saved at model/E18.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 91.7850, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 113.6914, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 96.1006, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 105.1953, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 84.1796, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 96.1239, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 106.5865, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 107.4902, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 98.2964, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 110.5972, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 99.7852, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 96.8777, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 87.9431, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 112.6710, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 138.8288, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 102.3304, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 93.2619, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 88.5943, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 94.1108, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 88.7710, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 83.5655, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 98.8133, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 91.9148, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 109.3194, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 102.2664, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 98.8695, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 96.6101, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 83.9808, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 114.7233, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 97.4857, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 120.1901, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 91.0746, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 95.3105, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 96.9777, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 81.4794, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 103.1544, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 87.0624, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 86.0167, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 96.1742, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 96.8067, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 99.0432, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 102.7223, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 107.5402, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 86.8908, \n",
      "TRAIN Epoch 19/20, NLL 97.3823, PPL 101.0753\n",
      "VALID Epoch 19/20, NLL 101.3506, PPL 128.0314\n",
      "TEST Epoch 19/20, NLL 100.6257, PPL 122.8178\n",
      "Model saved at model/E19.pkl\n",
      "\n",
      "Total cost time 00 hr 13 min 48 sec\n"
     ]
    }
   ],
   "source": [
    "# training interface\n",
    "step = 0\n",
    "tracker = {'NLL': []}\n",
    "start_time = time.time()\n",
    "for ep in range(epoch):\n",
    "    # learning rate decay\n",
    "    if (ep % 2 == 0) and (learning_rate>0.1):\n",
    "        learning_rate = learning_rate * 0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    for split in splits:\n",
    "        dataloader = dataloaders[split]\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        totals = {'NLL': 0., 'words': 0}\n",
    "\n",
    "        for itr, (_, dec_inputs, targets, lengths) in enumerate(dataloader):\n",
    "            bsize = dec_inputs.size(0)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # forward\n",
    "            logp = model(dec_inputs, lengths)\n",
    "\n",
    "            # calculate loss\n",
    "            NLL_loss = NLL(logp, targets, lengths + 1)\n",
    "            loss = NLL_loss / bsize\n",
    "\n",
    "            # cumulate\n",
    "            totals['NLL'] += NLL_loss.item()\n",
    "            totals['words'] += torch.sum(lengths).item()\n",
    "\n",
    "            # backward and optimize\n",
    "            if split == 'train':\n",
    "                step += 1\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 0.25) #5\n",
    "                optimizer.step()\n",
    "\n",
    "                # track\n",
    "                tracker['NLL'].append(loss.item())\n",
    "\n",
    "                # print statistics\n",
    "                if itr % print_every == 0 or itr + 1 == len(dataloader):\n",
    "                    print(\"%s Batch %04d/%04d, NLL-Loss %.4f, \"\n",
    "                          % (split.upper(), itr, len(dataloader),\n",
    "                             tracker['NLL'][-1]))\n",
    "\n",
    "        samples = len(datasets[split])\n",
    "        print(\"%s Epoch %02d/%02d, NLL %.4f, PPL %.4f\"\n",
    "              % (split.upper(), ep, epoch, totals['NLL'] / samples,\n",
    "                 math.exp(totals['NLL'] / totals['words'])))\n",
    "\n",
    "    # save checkpoint\n",
    "    checkpoint_path = os.path.join(save_path, \"E%02d.pkl\" % ep)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Model saved at %s\\n\" % checkpoint_path)\n",
    "end_time = time.time()\n",
    "print('Total cost time',\n",
    "      time.strftime(\"%H hr %M min %S sec\", time.gmtime(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning results\n",
    "sio.savemat(\"results.mat\", tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 8865102\n"
     ]
    }
   ],
   "source": [
    "print('# of parameters:', sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
