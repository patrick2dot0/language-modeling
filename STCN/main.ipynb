{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/god/python3-virtual/lib/python3.5/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from ptb import PTB\n",
    "from model import STCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU')\n",
    "else:\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penn TreeBank (PTB) dataset\n",
    "data_path = '../data'\n",
    "max_len = 96\n",
    "splits = ['train', 'valid', 'test']\n",
    "datasets = {split: PTB(root=data_path, split=split) for split in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "batch_size = 20 #32\n",
    "dataloaders = {split: DataLoader(datasets[split],\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=split=='train',\n",
    "                                 num_workers=cpu_count(),\n",
    "                                 pin_memory=torch.cuda.is_available())\n",
    "                                 for split in splits}\n",
    "symbols = datasets['train'].symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STCN(\n",
      "  (encoder): Embedding(10002, 300, padding_idx=0)\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(300, 450, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (conv2): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(300, 450, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.4, inplace=False)\n",
      "          (4): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.4, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(300, 450, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (conv2): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.4, inplace=False)\n",
      "          (4): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.4, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (conv2): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.4, inplace=False)\n",
      "          (4): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.4, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=1350, out_features=10002, bias=True)\n",
      "  (generative): GenerativeNetwork(\n",
      "    (network): Sequential(\n",
      "      (0): EncodeBlock(\n",
      "        (linear1): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (linear3): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (enc_mean): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_std): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): Softplus(beta=1, threshold=20)\n",
      "        )\n",
      "      )\n",
      "      (1): EncodeBlock(\n",
      "        (linear1): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (linear3): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (linear2): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_z): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (enc_mean): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_std): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): Softplus(beta=1, threshold=20)\n",
      "        )\n",
      "      )\n",
      "      (2): EncodeBlock(\n",
      "        (linear1): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (linear3): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (linear2): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_z): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (enc_mean): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_std): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): Softplus(beta=1, threshold=20)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (inference): InferenceNetwork(\n",
      "    (network): Sequential(\n",
      "      (0): EncodeBlock(\n",
      "        (linear1): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (linear3): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (enc_mean): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_std): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): Softplus(beta=1, threshold=20)\n",
      "        )\n",
      "      )\n",
      "      (1): EncodeBlock(\n",
      "        (linear1): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (linear3): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (linear2): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_z): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (enc_mean): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_std): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): Softplus(beta=1, threshold=20)\n",
      "        )\n",
      "      )\n",
      "      (2): EncodeBlock(\n",
      "        (linear1): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (linear3): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (linear2): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_z): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (enc_mean): Linear(in_features=450, out_features=450, bias=True)\n",
      "        (enc_std): Sequential(\n",
      "          (0): Linear(in_features=450, out_features=450, bias=True)\n",
      "          (1): Softplus(beta=1, threshold=20)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# STCN model\n",
    "embedding_size = 300 # dimension of character embeddings\n",
    "dropout_rate = 0.4\n",
    "emb_dropout_rate = 0.1\n",
    "levels = 3    # # of levels\n",
    "nhid = 450    # number of hidden units per layer\n",
    "num_chans = [nhid] * (levels)#[nhid] * (levels - 1) + [embedding_size]\n",
    "model = STCN(vocab_size=datasets['train'].vocab_size,\n",
    "            embed_size=embedding_size,\n",
    "            num_channels=num_chans,\n",
    "            bos_idx=symbols['<bos>'],\n",
    "            eos_idx=symbols['<eos>'],\n",
    "            pad_idx=symbols['<pad>'],\n",
    "            dropout=dropout_rate,\n",
    "            emb_dropout = emb_dropout_rate)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save model\n",
    "save_path = 'model'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/god/python3-virtual/lib/python3.5/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# objective function\n",
    "learning_rate = 4\n",
    "criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=symbols['<pad>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) #Adam\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate) #Adam\n",
    "\n",
    "\n",
    "# negative log likelihood\n",
    "def NLL(logp, target, length):\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp[:, :torch.max(length).item(),:].contiguous().view(-1, logp.size(-1)) # logp = logp.view(-1, logp.size(-1))\n",
    "    return criterion(logp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setting\n",
    "epoch = 20\n",
    "print_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/2104, NLL-Loss 213.6874, KL-Loss 0.0023, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 177.9694, KL-Loss 1.2142, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 209.5447, KL-Loss 4.1308, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 153.7311, KL-Loss 6.5192, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 158.4470, KL-Loss 9.0045, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 192.6690, KL-Loss 9.6218, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 148.7655, KL-Loss 11.4688, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 191.5117, KL-Loss 14.3553, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 158.5504, KL-Loss 13.1247, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 162.8992, KL-Loss 13.5499, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 166.5098, KL-Loss 13.2145, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 158.5920, KL-Loss 13.1861, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 134.4210, KL-Loss 16.0349, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 178.1622, KL-Loss 16.1133, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 162.0838, KL-Loss 16.2961, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 137.1474, KL-Loss 14.9918, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 141.2554, KL-Loss 16.7936, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 161.9234, KL-Loss 17.3259, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 187.0736, KL-Loss 16.9849, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 138.7559, KL-Loss 18.5642, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 163.9905, KL-Loss 17.3761, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 163.6875, KL-Loss 19.4679, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 208.9062, KL-Loss 20.8233, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 147.9920, KL-Loss 19.2660, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 135.4753, KL-Loss 18.9137, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 154.9633, KL-Loss 19.7759, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 126.7418, KL-Loss 18.8712, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 137.8270, KL-Loss 19.1015, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 137.2478, KL-Loss 19.0378, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 146.7186, KL-Loss 19.4121, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 158.6628, KL-Loss 19.3499, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 167.4579, KL-Loss 19.8433, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 143.1182, KL-Loss 19.7656, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 124.3469, KL-Loss 18.1184, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 155.2497, KL-Loss 19.2430, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 134.8208, KL-Loss 20.7756, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 149.9652, KL-Loss 19.8824, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 150.4742, KL-Loss 20.5191, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 171.9191, KL-Loss 20.0530, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 155.7480, KL-Loss 18.9839, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 144.0950, KL-Loss 19.7190, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 136.5154, KL-Loss 18.9123, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 135.4123, KL-Loss 19.7400, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 152.2179, KL-Loss 7.6597, \n",
      "TRAIN Epoch 00/20, NLL 152.7665, PPL 1395.5572\n",
      "VALID Epoch 00/20, NLL 131.6912, PPL 547.2269\n",
      "TEST Epoch 00/20, NLL 131.1696, PPL 528.9758\n",
      "Model saved at model/E00.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 141.6362, KL-Loss 18.9764, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 149.0370, KL-Loss 20.4498, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 127.2748, KL-Loss 18.3514, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 118.6597, KL-Loss 20.2134, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 149.9778, KL-Loss 18.6838, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 138.6180, KL-Loss 18.7237, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 145.8823, KL-Loss 18.9782, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 144.6221, KL-Loss 17.5429, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 143.1757, KL-Loss 17.3556, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 127.6649, KL-Loss 17.4888, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 122.5334, KL-Loss 16.9225, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 137.5273, KL-Loss 14.7132, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 143.3285, KL-Loss 14.5615, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 105.5705, KL-Loss 15.8663, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 122.6525, KL-Loss 14.5876, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 132.7854, KL-Loss 13.9352, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 136.3390, KL-Loss 13.9402, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 144.8061, KL-Loss 12.9380, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 128.7814, KL-Loss 12.7331, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 139.8777, KL-Loss 12.5996, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 137.3887, KL-Loss 13.4954, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 149.9993, KL-Loss 13.1955, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 141.6992, KL-Loss 12.0425, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 158.2418, KL-Loss 10.4243, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 137.4535, KL-Loss 11.6610, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 135.9711, KL-Loss 10.8862, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 137.8144, KL-Loss 12.4702, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 120.7288, KL-Loss 11.3518, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 123.0654, KL-Loss 10.3033, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 150.3210, KL-Loss 10.8940, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 111.1435, KL-Loss 9.6519, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 113.6383, KL-Loss 10.4944, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 150.7759, KL-Loss 10.9884, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 153.1559, KL-Loss 10.6754, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 156.5738, KL-Loss 11.1136, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 125.4697, KL-Loss 11.8410, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 124.4919, KL-Loss 10.8636, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 161.1937, KL-Loss 10.0106, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 131.2435, KL-Loss 11.3879, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 140.3141, KL-Loss 9.5375, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 124.6534, KL-Loss 10.2933, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 114.5608, KL-Loss 10.2316, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 141.1464, KL-Loss 11.1319, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 119.0943, KL-Loss 4.2285, \n",
      "TRAIN Epoch 01/20, NLL 135.5102, PPL 615.9150\n",
      "VALID Epoch 01/20, NLL 122.7029, PPL 355.8604\n",
      "TEST Epoch 01/20, NLL 122.1323, PPL 343.3976\n",
      "Model saved at model/E01.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 115.4692, KL-Loss 12.0621, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 93.6274, KL-Loss 11.7206, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 137.4379, KL-Loss 11.8654, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 141.7637, KL-Loss 12.0287, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 140.2339, KL-Loss 10.8383, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 127.1333, KL-Loss 10.8796, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 107.8596, KL-Loss 11.9781, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 125.9827, KL-Loss 11.9445, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 129.0446, KL-Loss 11.6467, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 132.1491, KL-Loss 10.9941, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 139.0603, KL-Loss 11.1917, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 100.3423, KL-Loss 11.4021, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 129.0562, KL-Loss 11.5469, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 113.2913, KL-Loss 12.1339, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 119.0764, KL-Loss 11.5538, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 129.1680, KL-Loss 11.6723, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 111.8194, KL-Loss 12.1294, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 119.0533, KL-Loss 11.8062, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 124.0682, KL-Loss 11.5571, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 132.4177, KL-Loss 11.1972, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 141.2453, KL-Loss 12.3817, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 120.6524, KL-Loss 10.7045, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 135.6299, KL-Loss 12.5140, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 151.3498, KL-Loss 11.7840, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 151.1275, KL-Loss 11.7019, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 121.4673, KL-Loss 12.8290, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 125.4922, KL-Loss 12.6844, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 133.3181, KL-Loss 11.1107, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 111.1290, KL-Loss 11.4426, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 155.9900, KL-Loss 11.5908, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 119.3054, KL-Loss 11.1941, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 117.6396, KL-Loss 11.2621, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 128.2029, KL-Loss 10.6635, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 137.1786, KL-Loss 11.1561, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 106.1186, KL-Loss 11.5899, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 110.8414, KL-Loss 11.8117, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 126.6924, KL-Loss 11.4427, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 121.2776, KL-Loss 11.6599, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 127.8727, KL-Loss 11.6396, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 146.8369, KL-Loss 12.5436, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 135.3834, KL-Loss 12.5842, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 129.9888, KL-Loss 11.0637, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 112.1076, KL-Loss 11.7727, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 138.4381, KL-Loss 4.5497, \n",
      "TRAIN Epoch 02/20, NLL 128.7110, PPL 446.2276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID Epoch 02/20, NLL 118.0831, PPL 285.2490\n",
      "TEST Epoch 02/20, NLL 117.5684, PPL 276.0828\n",
      "Model saved at model/E02.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 100.4766, KL-Loss 10.9670, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 132.9491, KL-Loss 14.1902, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 105.1902, KL-Loss 11.6031, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 123.3560, KL-Loss 12.2202, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 139.1994, KL-Loss 13.2129, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 134.4068, KL-Loss 11.1589, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 132.3535, KL-Loss 12.2258, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 126.4219, KL-Loss 12.0494, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 118.0922, KL-Loss 12.4528, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 124.1716, KL-Loss 12.4612, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 140.1402, KL-Loss 12.9144, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 135.3824, KL-Loss 12.1504, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 135.9822, KL-Loss 12.8087, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 107.5965, KL-Loss 11.9204, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 111.8579, KL-Loss 14.5044, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 125.9597, KL-Loss 12.2503, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 141.2191, KL-Loss 11.5228, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 131.9201, KL-Loss 13.2551, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 122.0162, KL-Loss 13.8155, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 134.8740, KL-Loss 12.0961, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 138.4905, KL-Loss 12.9606, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 130.8294, KL-Loss 11.9388, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 111.6690, KL-Loss 11.8355, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 143.0935, KL-Loss 13.2151, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 119.0090, KL-Loss 11.1141, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 135.3099, KL-Loss 12.1168, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 124.3723, KL-Loss 14.1983, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 144.7530, KL-Loss 11.2605, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 100.2204, KL-Loss 12.1885, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 124.9601, KL-Loss 14.1511, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 129.2422, KL-Loss 13.1382, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 135.2343, KL-Loss 13.1284, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 102.1172, KL-Loss 12.3166, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 119.4601, KL-Loss 12.6236, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 115.2549, KL-Loss 13.0040, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 107.0324, KL-Loss 12.0244, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 135.3492, KL-Loss 12.6662, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 122.1490, KL-Loss 13.4979, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 110.6873, KL-Loss 12.3373, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 118.5398, KL-Loss 12.7055, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 104.5753, KL-Loss 12.3741, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 110.7005, KL-Loss 12.4136, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 141.9590, KL-Loss 11.7009, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 137.5801, KL-Loss 5.1515, \n",
      "TRAIN Epoch 03/20, NLL 124.4836, PPL 365.2036\n",
      "VALID Epoch 03/20, NLL 115.1401, PPL 247.7599\n",
      "TEST Epoch 03/20, NLL 114.8541, PPL 242.4833\n",
      "Model saved at model/E03.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 105.7575, KL-Loss 12.6047, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 128.1737, KL-Loss 12.4746, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 102.3126, KL-Loss 12.6344, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 130.4747, KL-Loss 12.9046, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 137.6940, KL-Loss 12.4909, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 128.9507, KL-Loss 13.6775, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 122.6656, KL-Loss 14.3181, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 141.1547, KL-Loss 13.3849, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 137.7126, KL-Loss 13.8899, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 113.5394, KL-Loss 12.8155, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 115.4609, KL-Loss 13.1414, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 122.7177, KL-Loss 12.9606, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 141.3499, KL-Loss 13.8259, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 104.5246, KL-Loss 12.1494, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 149.8954, KL-Loss 12.8702, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 118.4445, KL-Loss 12.6014, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 108.6174, KL-Loss 13.6075, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 130.4462, KL-Loss 12.1723, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 123.5219, KL-Loss 13.2595, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 132.3986, KL-Loss 12.1272, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 111.0828, KL-Loss 13.7471, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 90.5835, KL-Loss 12.4451, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 104.0260, KL-Loss 13.5189, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 135.1867, KL-Loss 12.6528, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 102.4841, KL-Loss 13.8087, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 129.2460, KL-Loss 13.9217, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 131.0732, KL-Loss 13.3888, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 145.9675, KL-Loss 12.9744, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 149.4868, KL-Loss 13.4342, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 142.7359, KL-Loss 15.3991, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 128.8038, KL-Loss 13.7346, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 106.0707, KL-Loss 12.6381, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 122.1031, KL-Loss 14.5195, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 99.5979, KL-Loss 12.2886, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 113.7522, KL-Loss 13.7390, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 121.4321, KL-Loss 13.4742, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 105.5100, KL-Loss 12.5721, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 131.6005, KL-Loss 12.3220, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 127.9755, KL-Loss 13.1271, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 124.9406, KL-Loss 13.4645, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 116.0489, KL-Loss 14.1288, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 140.2321, KL-Loss 14.6993, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 110.5053, KL-Loss 13.8430, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 127.3752, KL-Loss 5.9066, \n",
      "TRAIN Epoch 04/20, NLL 121.2514, PPL 313.3270\n",
      "VALID Epoch 04/20, NLL 112.6263, PPL 219.6668\n",
      "TEST Epoch 04/20, NLL 112.1539, PPL 213.1173\n",
      "Model saved at model/E04.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 127.6024, KL-Loss 14.4624, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 107.2377, KL-Loss 12.6491, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 135.1392, KL-Loss 15.2868, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 109.0695, KL-Loss 13.2165, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 135.5891, KL-Loss 13.3705, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 128.7746, KL-Loss 15.3094, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 139.7896, KL-Loss 16.0636, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 145.2393, KL-Loss 15.6825, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 116.1986, KL-Loss 14.6130, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 124.6653, KL-Loss 15.0597, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 121.6560, KL-Loss 13.9420, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 111.7838, KL-Loss 15.4023, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 93.3866, KL-Loss 15.1215, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 162.0230, KL-Loss 14.1969, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 132.2873, KL-Loss 15.9784, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 116.4066, KL-Loss 12.8860, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 125.5698, KL-Loss 13.9013, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 107.1576, KL-Loss 15.0192, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 124.8967, KL-Loss 13.3720, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 125.8501, KL-Loss 13.8400, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 132.2462, KL-Loss 14.3638, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 122.9439, KL-Loss 14.2479, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 117.2659, KL-Loss 15.0650, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 116.8187, KL-Loss 14.2559, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 105.7775, KL-Loss 13.5768, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 106.6969, KL-Loss 14.6007, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 142.6182, KL-Loss 14.0109, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 93.4580, KL-Loss 13.8160, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 122.7013, KL-Loss 13.3263, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 116.9767, KL-Loss 14.3573, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 109.1662, KL-Loss 12.9448, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 121.3478, KL-Loss 14.6390, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 111.0900, KL-Loss 13.7332, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 122.2039, KL-Loss 13.3837, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 133.0250, KL-Loss 14.2957, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 136.5418, KL-Loss 15.7826, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 126.6299, KL-Loss 13.3692, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 112.8249, KL-Loss 12.8774, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 141.6429, KL-Loss 14.2481, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 122.3920, KL-Loss 15.3752, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 124.6978, KL-Loss 15.7352, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 120.3337, KL-Loss 14.9942, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 2100/2104, NLL-Loss 124.1607, KL-Loss 13.9838, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 151.5496, KL-Loss 5.1151, \n",
      "TRAIN Epoch 05/20, NLL 118.7888, PPL 278.8084\n",
      "VALID Epoch 05/20, NLL 111.0065, PPL 203.2751\n",
      "TEST Epoch 05/20, NLL 110.4232, PPL 196.1931\n",
      "Model saved at model/E05.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 101.8682, KL-Loss 14.0994, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 107.6651, KL-Loss 13.6556, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 97.7072, KL-Loss 15.1031, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 117.1303, KL-Loss 15.1650, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 110.2420, KL-Loss 14.2601, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 119.8236, KL-Loss 15.1619, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 121.3206, KL-Loss 14.2007, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 105.9515, KL-Loss 14.1247, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 97.9397, KL-Loss 15.5557, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 89.2013, KL-Loss 14.4863, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 129.0923, KL-Loss 15.4910, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 130.0247, KL-Loss 15.5536, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 125.8172, KL-Loss 14.1948, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 110.5848, KL-Loss 14.2487, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 120.0782, KL-Loss 14.7799, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 101.3675, KL-Loss 15.3463, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 115.7326, KL-Loss 13.7261, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 109.2688, KL-Loss 15.7310, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 113.2129, KL-Loss 14.4364, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 113.4987, KL-Loss 14.5235, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 123.7742, KL-Loss 15.6535, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 130.6106, KL-Loss 14.3272, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 114.2335, KL-Loss 14.8432, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 157.1772, KL-Loss 15.5319, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 95.6239, KL-Loss 13.7136, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 112.7458, KL-Loss 14.6951, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 128.1975, KL-Loss 15.6283, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 108.2229, KL-Loss 14.7049, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 107.2999, KL-Loss 15.5221, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 119.5628, KL-Loss 15.8721, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 113.9809, KL-Loss 14.8418, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 80.2376, KL-Loss 13.0129, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 102.9233, KL-Loss 15.7640, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 114.5380, KL-Loss 13.9215, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 100.2310, KL-Loss 15.7450, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 111.2127, KL-Loss 14.0439, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 132.1259, KL-Loss 14.7440, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 112.0096, KL-Loss 15.8250, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 115.7730, KL-Loss 16.1151, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 111.7315, KL-Loss 14.7110, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 123.0426, KL-Loss 14.7622, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 100.3410, KL-Loss 13.6138, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 113.6595, KL-Loss 14.2527, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 80.2289, KL-Loss 6.5274, \n",
      "TRAIN Epoch 06/20, NLL 116.6850, PPL 252.3469\n",
      "VALID Epoch 06/20, NLL 110.4263, PPL 197.7067\n",
      "TEST Epoch 06/20, NLL 109.8415, PPL 190.8124\n",
      "Model saved at model/E06.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 113.7874, KL-Loss 15.2680, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 108.8328, KL-Loss 15.7128, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 102.8970, KL-Loss 14.2114, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 107.9732, KL-Loss 15.8564, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 119.2989, KL-Loss 16.9592, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 122.2212, KL-Loss 16.3129, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 123.9837, KL-Loss 16.1614, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 104.8270, KL-Loss 15.4053, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 121.2398, KL-Loss 15.8555, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 108.1915, KL-Loss 15.7177, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 105.2895, KL-Loss 15.7779, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 116.3760, KL-Loss 18.1186, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 119.4924, KL-Loss 15.7494, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 134.7734, KL-Loss 18.3511, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 99.6093, KL-Loss 17.0286, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 117.1216, KL-Loss 15.6112, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 124.4448, KL-Loss 14.9833, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 120.7969, KL-Loss 15.0996, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 106.4110, KL-Loss 14.4725, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 133.1571, KL-Loss 16.0724, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 138.5298, KL-Loss 17.0615, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 126.1154, KL-Loss 15.6080, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 91.4699, KL-Loss 15.0404, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 131.1443, KL-Loss 16.4600, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 105.0672, KL-Loss 16.3137, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 114.5809, KL-Loss 15.5559, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 95.4104, KL-Loss 15.0673, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 102.1932, KL-Loss 14.5166, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 109.5886, KL-Loss 14.7423, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 122.6316, KL-Loss 14.9176, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 111.9422, KL-Loss 15.6003, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 119.6861, KL-Loss 16.9875, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 116.7281, KL-Loss 15.0648, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 115.9209, KL-Loss 16.0613, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 118.6831, KL-Loss 16.2477, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 107.4131, KL-Loss 14.6894, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 110.9961, KL-Loss 14.3073, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 99.9000, KL-Loss 15.4611, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 111.7288, KL-Loss 15.8227, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 112.1105, KL-Loss 15.5429, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 120.9453, KL-Loss 15.9479, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 102.4488, KL-Loss 15.2728, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 126.1921, KL-Loss 15.6851, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 90.2994, KL-Loss 5.7433, \n",
      "TRAIN Epoch 07/20, NLL 114.7595, PPL 230.3357\n",
      "VALID Epoch 07/20, NLL 109.3158, PPL 187.4691\n",
      "TEST Epoch 07/20, NLL 108.7568, PPL 181.1697\n",
      "Model saved at model/E07.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 117.2875, KL-Loss 16.1040, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 127.5868, KL-Loss 18.2341, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 93.8901, KL-Loss 15.9247, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 101.4664, KL-Loss 16.9732, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 112.2066, KL-Loss 16.6644, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 91.3205, KL-Loss 16.3255, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 103.2916, KL-Loss 17.4801, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 73.2597, KL-Loss 16.0953, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 125.8836, KL-Loss 16.8150, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 115.2617, KL-Loss 16.9267, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 122.7132, KL-Loss 17.4310, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 93.9625, KL-Loss 14.2087, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 133.3027, KL-Loss 16.5573, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 117.8455, KL-Loss 16.1278, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 99.5508, KL-Loss 16.6440, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 116.3419, KL-Loss 16.3432, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 96.6747, KL-Loss 14.9100, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 126.0717, KL-Loss 16.7969, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 100.7253, KL-Loss 14.9488, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 119.9638, KL-Loss 16.0935, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 117.4042, KL-Loss 17.1902, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 117.6927, KL-Loss 16.5274, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 93.9582, KL-Loss 16.3554, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 116.0526, KL-Loss 17.1114, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 121.2455, KL-Loss 17.7496, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 102.6503, KL-Loss 16.5855, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 124.3233, KL-Loss 15.8764, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 112.0692, KL-Loss 16.3849, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 112.2940, KL-Loss 15.9160, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 90.2519, KL-Loss 16.1112, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 105.6664, KL-Loss 16.2296, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 134.5334, KL-Loss 18.4624, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 127.6192, KL-Loss 17.7752, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 138.4937, KL-Loss 17.3747, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 114.0152, KL-Loss 16.9156, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 101.6597, KL-Loss 16.8609, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 113.1103, KL-Loss 16.8183, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 106.3953, KL-Loss 14.8898, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 117.1331, KL-Loss 16.9373, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1950/2104, NLL-Loss 100.6757, KL-Loss 16.0575, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 110.5975, KL-Loss 16.0856, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 112.3228, KL-Loss 14.8676, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 109.3163, KL-Loss 19.2919, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 125.7295, KL-Loss 6.9511, \n",
      "TRAIN Epoch 08/20, NLL 113.1405, PPL 213.3205\n",
      "VALID Epoch 08/20, NLL 108.7406, PPL 182.3775\n",
      "TEST Epoch 08/20, NLL 108.2353, PPL 176.7091\n",
      "Model saved at model/E08.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 118.0530, KL-Loss 16.7398, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 114.7296, KL-Loss 18.2685, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 112.4033, KL-Loss 16.3792, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 105.4946, KL-Loss 17.8274, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 119.4603, KL-Loss 17.4756, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 95.0120, KL-Loss 18.9807, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 120.4565, KL-Loss 17.4881, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 103.1412, KL-Loss 17.6078, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 89.5349, KL-Loss 14.7156, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 94.8564, KL-Loss 15.9637, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 123.8932, KL-Loss 17.9265, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 80.5945, KL-Loss 18.3717, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 118.7032, KL-Loss 17.0536, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 114.5713, KL-Loss 18.2999, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 112.9916, KL-Loss 16.6926, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 107.1749, KL-Loss 17.5497, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 106.1792, KL-Loss 18.8194, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 88.2846, KL-Loss 17.0528, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 128.7606, KL-Loss 17.4566, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 121.6217, KL-Loss 18.1864, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 127.7162, KL-Loss 17.0570, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 96.6596, KL-Loss 16.3196, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 93.7170, KL-Loss 15.9609, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 111.8478, KL-Loss 17.7264, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 98.0578, KL-Loss 16.7508, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 104.4964, KL-Loss 17.9631, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 127.0108, KL-Loss 18.3889, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 122.0311, KL-Loss 17.4799, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 101.0243, KL-Loss 17.0034, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 105.8447, KL-Loss 16.8187, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 126.6878, KL-Loss 17.9450, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 117.0740, KL-Loss 16.8630, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 131.2122, KL-Loss 16.5726, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 103.6986, KL-Loss 17.2067, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 104.1072, KL-Loss 17.0612, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 107.8949, KL-Loss 17.4847, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 127.9243, KL-Loss 17.3232, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 117.0073, KL-Loss 17.5350, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 91.9344, KL-Loss 15.3467, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 108.2799, KL-Loss 17.0747, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 118.5889, KL-Loss 17.3557, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 116.1834, KL-Loss 17.3603, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 109.5353, KL-Loss 17.2596, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 111.3751, KL-Loss 7.7260, \n",
      "TRAIN Epoch 09/20, NLL 111.7250, PPL 199.4776\n",
      "VALID Epoch 09/20, NLL 108.0358, PPL 176.3260\n",
      "TEST Epoch 09/20, NLL 107.4466, PPL 170.1700\n",
      "Model saved at model/E09.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 115.5189, KL-Loss 17.3526, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 107.7126, KL-Loss 18.4270, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 114.5279, KL-Loss 20.0848, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 96.9134, KL-Loss 19.5946, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 106.6365, KL-Loss 21.2822, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 103.6426, KL-Loss 20.3132, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 85.6293, KL-Loss 18.6350, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 108.7546, KL-Loss 22.4606, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 94.9836, KL-Loss 19.9934, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 111.4405, KL-Loss 19.4584, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 115.6344, KL-Loss 19.1676, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 136.7682, KL-Loss 22.3432, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 98.7387, KL-Loss 19.1059, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 114.2156, KL-Loss 19.7449, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 92.8640, KL-Loss 19.6146, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 114.1326, KL-Loss 20.6545, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 116.5316, KL-Loss 20.6144, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 96.3175, KL-Loss 19.5310, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 114.8643, KL-Loss 20.7548, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 96.8400, KL-Loss 19.5431, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 104.1276, KL-Loss 19.7150, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 128.2590, KL-Loss 20.9036, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 101.4242, KL-Loss 18.6941, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 111.5390, KL-Loss 21.5843, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 109.0876, KL-Loss 19.6939, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 100.0830, KL-Loss 21.4524, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 106.3020, KL-Loss 20.7990, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 109.0308, KL-Loss 20.0297, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 94.8591, KL-Loss 19.8687, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 104.6628, KL-Loss 21.1390, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 100.5068, KL-Loss 21.5063, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 135.6363, KL-Loss 20.4610, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 99.7446, KL-Loss 19.3531, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 118.2406, KL-Loss 21.2697, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 107.6066, KL-Loss 21.2068, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 123.3971, KL-Loss 21.3832, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 107.0888, KL-Loss 21.2715, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 123.6747, KL-Loss 21.1348, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 105.6632, KL-Loss 19.2964, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 113.0688, KL-Loss 20.6673, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 103.9915, KL-Loss 19.3058, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 99.3956, KL-Loss 21.4073, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 123.7636, KL-Loss 20.3499, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 139.2015, KL-Loss 8.1659, \n",
      "TRAIN Epoch 10/20, NLL 107.8997, PPL 166.3983\n",
      "VALID Epoch 10/20, NLL 105.7780, PPL 158.2598\n",
      "TEST Epoch 10/20, NLL 105.0334, PPL 151.6274\n",
      "Model saved at model/E10.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 92.4213, KL-Loss 19.9822, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 98.1633, KL-Loss 21.7667, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 119.6417, KL-Loss 21.1708, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 115.1727, KL-Loss 20.2980, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 98.6263, KL-Loss 20.3876, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 106.5676, KL-Loss 22.8736, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 114.9594, KL-Loss 23.2603, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 107.3515, KL-Loss 20.7989, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 110.4504, KL-Loss 22.4097, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 118.4809, KL-Loss 24.4051, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 114.2340, KL-Loss 22.4807, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 102.9805, KL-Loss 21.2978, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 118.1845, KL-Loss 22.1520, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 106.7430, KL-Loss 22.2818, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 109.1757, KL-Loss 21.6824, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 114.6910, KL-Loss 21.8859, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 99.1072, KL-Loss 21.4385, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 106.0686, KL-Loss 22.7385, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 101.4750, KL-Loss 20.9847, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 130.4163, KL-Loss 21.9177, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 84.7620, KL-Loss 19.8094, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 103.7299, KL-Loss 20.8335, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 100.9616, KL-Loss 22.5180, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 92.0386, KL-Loss 20.1457, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 99.2057, KL-Loss 20.8694, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 109.9089, KL-Loss 21.2939, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 117.0414, KL-Loss 22.1154, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 109.5784, KL-Loss 21.6776, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 106.2595, KL-Loss 21.0233, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 121.6994, KL-Loss 22.1022, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 107.2625, KL-Loss 21.3464, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 100.6315, KL-Loss 20.4942, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 107.0869, KL-Loss 20.7085, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 122.5597, KL-Loss 22.3386, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 101.8031, KL-Loss 20.4533, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 95.2326, KL-Loss 19.5878, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1800/2104, NLL-Loss 106.5377, KL-Loss 21.5133, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 121.7836, KL-Loss 21.3907, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 105.2835, KL-Loss 21.7708, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 109.2510, KL-Loss 20.2747, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 106.3537, KL-Loss 21.0532, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 114.1665, KL-Loss 23.5616, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 92.1216, KL-Loss 21.9809, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 84.1608, KL-Loss 8.7483, \n",
      "TRAIN Epoch 11/20, NLL 106.7919, PPL 157.8861\n",
      "VALID Epoch 11/20, NLL 105.5202, PPL 156.3187\n",
      "TEST Epoch 11/20, NLL 104.8050, PPL 149.9810\n",
      "Model saved at model/E11.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 95.6496, KL-Loss 22.3237, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 86.7402, KL-Loss 21.2024, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 110.7623, KL-Loss 23.4717, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 114.0110, KL-Loss 25.1889, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 109.5431, KL-Loss 24.1470, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 118.6796, KL-Loss 22.6646, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 100.8582, KL-Loss 24.2505, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 95.7182, KL-Loss 23.0632, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 120.6917, KL-Loss 24.2336, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 98.5543, KL-Loss 22.6329, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 74.6500, KL-Loss 22.7378, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 110.9974, KL-Loss 24.2314, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 91.7412, KL-Loss 21.9009, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 109.8151, KL-Loss 23.7377, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 94.6814, KL-Loss 24.0917, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 106.1963, KL-Loss 24.5717, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 105.0884, KL-Loss 22.8718, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 84.8505, KL-Loss 21.4120, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 100.3247, KL-Loss 23.4168, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 101.8774, KL-Loss 25.0604, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 105.5962, KL-Loss 22.9821, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 109.7287, KL-Loss 21.7557, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 95.7638, KL-Loss 23.4577, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 94.6105, KL-Loss 22.9275, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 98.2770, KL-Loss 24.0746, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 101.6712, KL-Loss 22.6848, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 100.7814, KL-Loss 22.3948, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 82.0142, KL-Loss 22.4208, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 103.0018, KL-Loss 22.6637, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 113.4434, KL-Loss 25.0644, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 119.9551, KL-Loss 23.0560, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 91.0405, KL-Loss 20.6716, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 120.4938, KL-Loss 24.0345, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 108.4463, KL-Loss 23.7732, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 99.8347, KL-Loss 21.8741, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 93.3463, KL-Loss 24.5375, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 97.1785, KL-Loss 23.2884, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 101.8660, KL-Loss 24.4923, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 86.4412, KL-Loss 22.2858, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 108.3018, KL-Loss 22.3641, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 120.1848, KL-Loss 24.2164, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 90.1429, KL-Loss 23.2162, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 104.2274, KL-Loss 22.0195, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 92.0670, KL-Loss 9.5799, \n",
      "TRAIN Epoch 12/20, NLL 104.6656, PPL 142.7497\n",
      "VALID Epoch 12/20, NLL 104.6874, PPL 150.2087\n",
      "TEST Epoch 12/20, NLL 104.0142, PPL 144.4161\n",
      "Model saved at model/E12.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 107.5606, KL-Loss 22.3061, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 113.3801, KL-Loss 26.0248, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 116.1404, KL-Loss 25.6321, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 97.1426, KL-Loss 23.6031, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 97.2216, KL-Loss 24.1224, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 103.9623, KL-Loss 25.6284, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 111.1374, KL-Loss 26.0782, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 103.8064, KL-Loss 25.0468, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 115.2539, KL-Loss 25.6791, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 115.2263, KL-Loss 24.1675, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 85.8220, KL-Loss 23.4246, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 89.2768, KL-Loss 24.5381, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 107.7687, KL-Loss 23.9725, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 98.2389, KL-Loss 25.3043, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 105.6491, KL-Loss 24.4036, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 106.1638, KL-Loss 25.4503, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 115.3726, KL-Loss 26.0032, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 109.9529, KL-Loss 24.7595, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 105.3321, KL-Loss 24.4629, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 107.0485, KL-Loss 24.2496, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 107.9309, KL-Loss 24.5503, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 115.5722, KL-Loss 24.1070, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 102.0441, KL-Loss 24.1312, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 107.3762, KL-Loss 26.1257, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 106.0034, KL-Loss 24.6674, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 130.2404, KL-Loss 24.8579, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 97.7631, KL-Loss 23.2239, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 105.8211, KL-Loss 23.8115, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 105.9038, KL-Loss 24.5595, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 115.7219, KL-Loss 24.4699, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 104.1331, KL-Loss 24.1556, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 120.6072, KL-Loss 25.9032, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 97.3109, KL-Loss 24.4967, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 102.8307, KL-Loss 25.5932, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 94.3486, KL-Loss 25.8577, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 101.5701, KL-Loss 23.5850, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 93.8151, KL-Loss 23.9465, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 106.7205, KL-Loss 23.2708, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 124.9296, KL-Loss 25.6373, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 102.5508, KL-Loss 22.9269, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 89.7654, KL-Loss 23.6270, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 128.7586, KL-Loss 22.0753, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 110.6952, KL-Loss 24.4602, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 126.3324, KL-Loss 9.8163, \n",
      "TRAIN Epoch 13/20, NLL 104.0078, PPL 138.3673\n",
      "VALID Epoch 13/20, NLL 104.5861, PPL 149.4821\n",
      "TEST Epoch 13/20, NLL 103.8869, PPL 143.5401\n",
      "Model saved at model/E13.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 100.0650, KL-Loss 25.2825, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 105.9837, KL-Loss 25.6906, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 115.8073, KL-Loss 24.5731, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 119.5403, KL-Loss 24.5453, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 108.2152, KL-Loss 26.2397, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 110.9550, KL-Loss 25.8217, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 96.7063, KL-Loss 25.5405, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 82.8534, KL-Loss 23.2191, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 101.4801, KL-Loss 25.4630, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 98.8366, KL-Loss 25.1016, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 118.0051, KL-Loss 25.8125, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 85.9999, KL-Loss 21.1902, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 87.3785, KL-Loss 26.5049, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 79.4205, KL-Loss 25.4526, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 99.3820, KL-Loss 26.1636, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 98.7980, KL-Loss 26.9516, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 100.4310, KL-Loss 24.8801, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 120.4887, KL-Loss 25.8033, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 79.8635, KL-Loss 24.5505, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 96.2459, KL-Loss 26.2368, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 103.5890, KL-Loss 25.0291, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 105.8387, KL-Loss 25.3169, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 111.1695, KL-Loss 26.4554, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 95.7566, KL-Loss 27.2036, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 123.2121, KL-Loss 25.1162, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 83.4895, KL-Loss 21.4898, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 111.7875, KL-Loss 24.9698, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 109.0407, KL-Loss 25.5915, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 88.3800, KL-Loss 22.6100, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 103.9300, KL-Loss 25.3076, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 96.7673, KL-Loss 26.1274, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 93.7038, KL-Loss 25.2585, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 115.8382, KL-Loss 25.3266, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 108.6365, KL-Loss 25.8652, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1700/2104, NLL-Loss 113.9020, KL-Loss 26.5436, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 85.0693, KL-Loss 24.7703, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 110.9129, KL-Loss 27.1475, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 125.4638, KL-Loss 27.1542, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 107.5491, KL-Loss 26.3185, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 95.0682, KL-Loss 25.2271, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 138.5541, KL-Loss 26.1550, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 101.4465, KL-Loss 27.1641, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 114.5428, KL-Loss 25.0872, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 109.6765, KL-Loss 10.5585, \n",
      "TRAIN Epoch 14/20, NLL 102.8154, PPL 130.7639\n",
      "VALID Epoch 14/20, NLL 104.3530, PPL 147.8233\n",
      "TEST Epoch 14/20, NLL 103.6138, PPL 141.6781\n",
      "Model saved at model/E14.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 84.9481, KL-Loss 23.3886, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 98.7855, KL-Loss 25.3492, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 103.3992, KL-Loss 27.9790, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 132.2521, KL-Loss 26.9834, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 123.3665, KL-Loss 27.2423, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 109.9443, KL-Loss 25.8208, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 97.5155, KL-Loss 26.0619, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 96.0427, KL-Loss 25.6311, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 116.1613, KL-Loss 27.4952, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 107.8677, KL-Loss 26.1471, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 102.7445, KL-Loss 27.9350, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 92.2188, KL-Loss 25.8876, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 87.1068, KL-Loss 25.5636, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 102.9931, KL-Loss 24.9809, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 110.2703, KL-Loss 25.3098, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 83.8395, KL-Loss 25.1973, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 83.9060, KL-Loss 25.3128, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 90.8141, KL-Loss 25.0577, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 90.3361, KL-Loss 26.8258, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 90.6139, KL-Loss 23.5401, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 103.1124, KL-Loss 26.5617, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 97.3567, KL-Loss 25.0495, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 85.7091, KL-Loss 26.2241, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 89.6317, KL-Loss 26.6455, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 115.7234, KL-Loss 25.7840, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 102.9834, KL-Loss 25.6711, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 122.8951, KL-Loss 26.4605, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 95.1638, KL-Loss 26.0851, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 101.2870, KL-Loss 25.3029, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 87.0898, KL-Loss 25.4818, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 94.9161, KL-Loss 25.1690, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 105.9211, KL-Loss 27.1360, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 110.6839, KL-Loss 26.3358, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 83.7317, KL-Loss 24.5069, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 75.0793, KL-Loss 23.3674, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 97.2994, KL-Loss 27.5447, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 106.4273, KL-Loss 26.2897, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 114.1426, KL-Loss 27.7856, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 99.8864, KL-Loss 26.5335, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 102.3713, KL-Loss 26.4589, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 108.0155, KL-Loss 25.8023, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 87.9682, KL-Loss 26.1031, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 102.5839, KL-Loss 26.4538, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 70.8822, KL-Loss 8.4651, \n",
      "TRAIN Epoch 15/20, NLL 102.3450, PPL 127.8806\n",
      "VALID Epoch 15/20, NLL 104.3522, PPL 147.8173\n",
      "TEST Epoch 15/20, NLL 103.5873, PPL 141.4987\n",
      "Model saved at model/E15.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 96.0891, KL-Loss 25.8285, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 79.6674, KL-Loss 23.8962, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 102.1915, KL-Loss 24.7542, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 92.8375, KL-Loss 26.2200, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 99.6623, KL-Loss 25.2998, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 104.6841, KL-Loss 25.4761, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 88.0168, KL-Loss 25.7607, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 101.3599, KL-Loss 25.9896, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 96.0876, KL-Loss 26.4072, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 107.0550, KL-Loss 25.6460, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 103.5709, KL-Loss 24.9778, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 129.8127, KL-Loss 27.2401, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 85.4169, KL-Loss 27.0627, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 96.8372, KL-Loss 26.4030, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 75.7686, KL-Loss 24.7377, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 105.4507, KL-Loss 26.9690, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 109.2264, KL-Loss 27.6264, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 99.0783, KL-Loss 26.3979, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 105.2069, KL-Loss 27.1411, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 94.0112, KL-Loss 26.8221, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 93.3394, KL-Loss 26.8669, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 103.8236, KL-Loss 25.6380, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 80.9778, KL-Loss 26.3230, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 111.2217, KL-Loss 27.1755, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 102.7313, KL-Loss 25.9803, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 112.2932, KL-Loss 27.0519, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 104.1033, KL-Loss 27.4427, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 125.8471, KL-Loss 27.0317, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 94.4534, KL-Loss 26.8625, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 102.2897, KL-Loss 25.1375, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 103.7921, KL-Loss 25.1837, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 100.6615, KL-Loss 29.3243, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 124.1093, KL-Loss 29.1804, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 95.1128, KL-Loss 26.9769, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 91.0606, KL-Loss 26.8417, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 93.3922, KL-Loss 24.8879, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 97.7010, KL-Loss 26.3191, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 126.3056, KL-Loss 28.8165, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 103.4282, KL-Loss 27.2661, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 93.0542, KL-Loss 27.1056, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 105.7876, KL-Loss 26.4359, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 100.5874, KL-Loss 25.9484, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 118.8218, KL-Loss 29.9352, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 116.0590, KL-Loss 11.2937, \n",
      "TRAIN Epoch 16/20, NLL 101.7152, PPL 124.1191\n",
      "VALID Epoch 16/20, NLL 104.3157, PPL 147.5595\n",
      "TEST Epoch 16/20, NLL 103.5338, PPL 141.1374\n",
      "Model saved at model/E16.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 94.2619, KL-Loss 24.1782, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 127.2691, KL-Loss 27.1610, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 114.3513, KL-Loss 27.7272, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 114.5041, KL-Loss 27.3803, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 94.9707, KL-Loss 27.1193, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 115.7240, KL-Loss 28.7645, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 84.3345, KL-Loss 26.5093, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 99.9669, KL-Loss 25.0801, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 101.2042, KL-Loss 28.0852, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 99.1741, KL-Loss 27.2008, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 102.8160, KL-Loss 26.1304, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 85.8073, KL-Loss 25.4000, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 97.0697, KL-Loss 25.3177, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 103.6569, KL-Loss 26.4420, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 84.3066, KL-Loss 26.1639, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 118.5353, KL-Loss 29.0492, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 101.4373, KL-Loss 28.2389, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 97.0798, KL-Loss 27.6352, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 101.9867, KL-Loss 27.3951, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 100.6611, KL-Loss 27.6752, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 95.0042, KL-Loss 26.5840, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 116.5735, KL-Loss 27.4104, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 102.3190, KL-Loss 28.5214, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 80.0667, KL-Loss 24.5363, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 83.2528, KL-Loss 26.0689, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 132.9598, KL-Loss 28.8687, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 111.3801, KL-Loss 27.8668, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 92.7508, KL-Loss 27.5943, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 103.0627, KL-Loss 28.1661, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 137.7812, KL-Loss 27.2057, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 91.8295, KL-Loss 25.5917, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 86.6230, KL-Loss 25.1410, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1600/2104, NLL-Loss 71.0324, KL-Loss 24.3764, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 90.0773, KL-Loss 26.1654, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 95.5534, KL-Loss 27.0190, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 116.6957, KL-Loss 27.7648, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 100.8306, KL-Loss 27.8169, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 120.3904, KL-Loss 30.1184, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 108.0135, KL-Loss 26.1729, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 112.2885, KL-Loss 27.6686, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 95.5196, KL-Loss 26.0156, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 75.3605, KL-Loss 24.3454, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 101.8739, KL-Loss 26.6925, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 84.4714, KL-Loss 10.7157, \n",
      "TRAIN Epoch 17/20, NLL 101.4257, PPL 122.4279\n",
      "VALID Epoch 17/20, NLL 104.2535, PPL 147.1209\n",
      "TEST Epoch 17/20, NLL 103.4711, PPL 140.7147\n",
      "Model saved at model/E17.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 116.7305, KL-Loss 27.7590, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 139.0717, KL-Loss 28.0671, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 92.5651, KL-Loss 28.9555, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 109.9415, KL-Loss 28.0420, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 106.1701, KL-Loss 27.1203, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 97.7817, KL-Loss 27.5386, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 111.5822, KL-Loss 28.3531, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 91.2756, KL-Loss 27.4534, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 96.7172, KL-Loss 26.6838, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 88.0384, KL-Loss 28.3557, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 102.7406, KL-Loss 28.7490, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 89.0593, KL-Loss 27.6533, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 114.9012, KL-Loss 28.5757, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 103.3762, KL-Loss 27.4360, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 77.4688, KL-Loss 27.4136, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 109.7743, KL-Loss 27.0985, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 100.2998, KL-Loss 26.3566, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 99.6529, KL-Loss 26.7949, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 104.6498, KL-Loss 26.8118, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 112.9954, KL-Loss 27.7039, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 90.1220, KL-Loss 29.1420, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 82.8004, KL-Loss 24.3873, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 114.0693, KL-Loss 28.7049, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 110.1280, KL-Loss 28.0654, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 87.9009, KL-Loss 25.5779, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 115.7870, KL-Loss 27.9214, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 87.8382, KL-Loss 24.9947, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 103.3172, KL-Loss 27.5219, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 85.4802, KL-Loss 28.4049, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 107.7364, KL-Loss 25.8862, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 72.7519, KL-Loss 24.7284, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 96.1168, KL-Loss 27.6639, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 97.5191, KL-Loss 27.5139, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 95.4699, KL-Loss 26.6240, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 111.3968, KL-Loss 25.8552, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 124.8217, KL-Loss 26.3978, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 90.5588, KL-Loss 26.7895, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 130.6238, KL-Loss 27.3571, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 115.5535, KL-Loss 26.6430, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 79.4666, KL-Loss 26.4802, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 95.1460, KL-Loss 28.8394, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 95.2402, KL-Loss 26.9298, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 104.1202, KL-Loss 26.6966, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 94.0738, KL-Loss 11.2282, \n",
      "TRAIN Epoch 18/20, NLL 101.1761, PPL 120.9881\n",
      "VALID Epoch 18/20, NLL 104.2828, PPL 147.3275\n",
      "TEST Epoch 18/20, NLL 103.4702, PPL 140.7092\n",
      "Model saved at model/E18.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 119.4990, KL-Loss 27.0366, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 122.8354, KL-Loss 27.9124, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 103.2624, KL-Loss 28.5597, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 87.5330, KL-Loss 27.6500, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 85.0989, KL-Loss 26.1246, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 115.5839, KL-Loss 26.8614, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 102.4808, KL-Loss 27.4467, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 122.1948, KL-Loss 28.1720, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 88.8530, KL-Loss 25.6696, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 103.4825, KL-Loss 28.3147, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 139.3846, KL-Loss 28.3399, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 83.8760, KL-Loss 27.6129, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 112.6057, KL-Loss 27.4436, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 100.8180, KL-Loss 27.6767, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 109.4337, KL-Loss 28.0498, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 95.8184, KL-Loss 27.4490, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 112.9806, KL-Loss 27.4232, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 107.8519, KL-Loss 27.7951, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 103.4047, KL-Loss 27.7488, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 87.7793, KL-Loss 25.3196, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 96.7538, KL-Loss 26.4153, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 88.6291, KL-Loss 27.7517, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 127.4717, KL-Loss 30.1005, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 97.5175, KL-Loss 28.2808, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 108.8103, KL-Loss 27.7440, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 94.7341, KL-Loss 27.2475, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 88.4444, KL-Loss 26.8225, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 99.9872, KL-Loss 27.6208, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 101.1507, KL-Loss 28.8674, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 89.3437, KL-Loss 29.1239, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 99.4094, KL-Loss 28.2711, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 125.0645, KL-Loss 28.7584, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 121.6109, KL-Loss 28.0538, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 88.5722, KL-Loss 26.5039, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 105.0098, KL-Loss 28.4836, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 96.1038, KL-Loss 27.3663, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 99.0775, KL-Loss 26.9146, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 107.4667, KL-Loss 27.2893, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 102.8471, KL-Loss 27.6733, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 110.5225, KL-Loss 29.3152, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 115.8642, KL-Loss 26.7226, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 103.7695, KL-Loss 27.3420, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 95.4168, KL-Loss 26.9190, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 97.9113, KL-Loss 11.3562, \n",
      "TRAIN Epoch 19/20, NLL 101.0407, PPL 120.2140\n",
      "VALID Epoch 19/20, NLL 104.2752, PPL 147.2737\n",
      "TEST Epoch 19/20, NLL 103.4507, PPL 140.5779\n",
      "Model saved at model/E19.pkl\n",
      "\n",
      "Total cost time 01 hr 27 min 11 sec\n"
     ]
    }
   ],
   "source": [
    "# training interface\n",
    "step = 0\n",
    "NLL_tracker = {'NLL': []}\n",
    "KL_tracker = {'KL': []}\n",
    "start_time = time.time()\n",
    "beta = 1 #1.0/epoch\n",
    "beta_increase = 0\n",
    "for ep in range(epoch):\n",
    "    # learning rate decay\n",
    "    if (ep % 2 == 0) and (learning_rate>0.1) and (ep>=10):\n",
    "        learning_rate = learning_rate * 0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    for split in splits:\n",
    "        dataloader = dataloaders[split]\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        totals = {'NLL': 0.,'KL': 0., 'words': 0}\n",
    "\n",
    "        for itr, (_, dec_inputs, targets, lengths) in enumerate(dataloader):\n",
    "            bsize = dec_inputs.size(0)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # forward\n",
    "            logp, NLL_loss, KL_loss = model(dec_inputs, lengths, targets) #, lengths\n",
    "\n",
    "            # calculate loss\n",
    "            #NLL_loss = NLL(logp, targets, lengths + 1)\n",
    "            KL_loss = KL_loss\n",
    "            loss = NLL_loss + beta*KL_loss\n",
    "\n",
    "            # cumulate\n",
    "            totals['NLL'] += NLL_loss.item()\n",
    "            totals['KL'] += KL_loss.item()\n",
    "            totals['words'] += torch.sum(lengths).item()\n",
    "            NLL_loss = NLL_loss / bsize\n",
    "\n",
    "            # backward and optimize\n",
    "            if split == 'train':\n",
    "                step += 1\n",
    "\n",
    "                # track\n",
    "                NLL_tracker['NLL'].append(NLL_loss.item())\n",
    "                KL_tracker['KL'].append(KL_loss.item())\n",
    "\n",
    "                # print statistics\n",
    "                if itr % print_every == 0 or itr + 1 == len(dataloader):\n",
    "                    print(\"%s Batch %04d/%04d, NLL-Loss %.4f, KL-Loss %.4f, \"\n",
    "                          % (split.upper(), itr, len(dataloader),\n",
    "                             NLL_tracker['NLL'][-1], KL_tracker['KL'][-1]))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 0.25) #5\n",
    "                optimizer.step()\n",
    "\n",
    "        samples = len(datasets[split])\n",
    "        print(\"%s Epoch %02d/%02d, NLL %.4f, PPL %.4f\"\n",
    "              % (split.upper(), ep, epoch, totals['NLL'] / samples,\n",
    "                 math.exp(totals['NLL'] / totals['words'])))\n",
    "    # KL annealing\n",
    "    #if beta<=1:\n",
    "    #    beta+=beta_increase\n",
    "\n",
    "    # save checkpoint\n",
    "    checkpoint_path = os.path.join(save_path, \"E%02d.pkl\" % ep)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Model saved at %s\\n\" % checkpoint_path)\n",
    "end_time = time.time()\n",
    "print('Total cost time',\n",
    "      time.strftime(\"%H hr %M min %S sec\", time.gmtime(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 23414052\n"
     ]
    }
   ],
   "source": [
    "print('# of parameters:', sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning results\n",
    "sio.savemat(\"results.mat\", NLL_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-virtual",
   "language": "python",
   "name": "python3-virtual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
