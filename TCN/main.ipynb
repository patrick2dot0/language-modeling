{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/god/python3-virtual/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from ptb import PTB\n",
    "from model import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU')\n",
    "else:\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penn TreeBank (PTB) dataset\n",
    "data_path = '../data'\n",
    "max_len = 96\n",
    "splits = ['train', 'valid', 'test']\n",
    "datasets = {split: PTB(root=data_path, split=split) for split in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "batch_size = 20 #32\n",
    "dataloaders = {split: DataLoader(datasets[split],\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=split=='train',\n",
    "                                 num_workers=cpu_count(),\n",
    "                                 pin_memory=torch.cuda.is_available())\n",
    "                                 for split in splits}\n",
    "symbols = datasets['train'].symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN(\n",
      "  (encoder): Embedding(10002, 300, padding_idx=0)\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(300, 450, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.1)\n",
      "        (conv2): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.1)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(300, 450, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.1)\n",
      "          (4): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.1)\n",
      "        )\n",
      "        (downsample): Conv1d(300, 450, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.1)\n",
      "        (conv2): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.1)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.1)\n",
      "          (4): Conv1d(450, 450, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.1)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(450, 300, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.1)\n",
      "        (conv2): Conv1d(300, 300, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.1)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(450, 300, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.1)\n",
      "          (4): Conv1d(300, 300, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.1)\n",
      "        )\n",
      "        (downsample): Conv1d(450, 300, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=300, out_features=10002, bias=True)\n",
      "  (drop): Dropout(p=0.1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TCN model\n",
    "embedding_size = 300 # dimension of character embeddings\n",
    "dropout_rate = 0.1\n",
    "emb_dropout_rate = 0.1\n",
    "levels = 3    # # of levels\n",
    "nhid = 450    # number of hidden units per layer\n",
    "num_chans = [nhid] * (levels - 1) + [embedding_size]\n",
    "model = TCN(vocab_size=datasets['train'].vocab_size,\n",
    "            embed_size=embedding_size,\n",
    "            num_channels=num_chans,\n",
    "            bos_idx=symbols['<bos>'],\n",
    "            eos_idx=symbols['<eos>'],\n",
    "            pad_idx=symbols['<pad>'],\n",
    "            dropout=dropout_rate,\n",
    "            emb_dropout = emb_dropout_rate)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save model\n",
    "save_path = 'model'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/god/python3-virtual/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# objective function\n",
    "learning_rate = 4\n",
    "criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=symbols['<pad>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) #Adam\n",
    "\n",
    "# negative log likelihood\n",
    "def NLL(logp, target, length):\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp[:, :torch.max(length).item(),:].contiguous().view(-1, logp.size(-1)) # logp = logp.view(-1, logp.size(-1))\n",
    "    return criterion(logp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setting\n",
    "epoch = 20\n",
    "print_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/2104, NLL-Loss 225.1025, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 145.5520, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 125.8556, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 162.6817, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 142.8749, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 145.2492, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 177.6509, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 154.1612, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 137.9166, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 131.1361, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 155.6833, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 132.4079, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 131.7718, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 128.9045, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 148.2869, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 160.0674, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 146.0038, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 159.9478, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 128.5474, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 143.5488, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 140.3116, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 139.0043, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 126.3233, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 129.6172, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 129.0791, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 141.7990, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 104.9932, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 130.2572, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 128.1622, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 162.9928, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 138.7404, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 138.4001, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 162.0451, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 127.6527, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 135.7638, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 125.6254, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 116.8351, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 160.9819, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 154.9783, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 137.0933, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 153.1493, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 127.9844, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 142.5746, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 129.8617, \n",
      "TRAIN Epoch 00/20, NLL 141.1972, PPL 806.4718\n",
      "VALID Epoch 00/20, NLL 122.8024, PPL 357.5601\n",
      "TEST Epoch 00/20, NLL 122.4309, PPL 348.3346\n",
      "Model saved at model/E00.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 133.5938, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 123.1295, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 139.3118, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 118.3558, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 123.9931, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 151.6366, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 130.0334, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 144.8209, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 129.3598, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 130.6541, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 129.3929, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 132.2350, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 137.9312, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 113.3231, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 149.9475, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 138.1849, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 130.5367, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 107.7839, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 110.7838, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 110.1654, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 108.3083, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 126.2012, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 144.1426, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 144.4333, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 131.6290, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 127.2079, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 123.2210, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 121.6619, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 133.9408, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 137.2312, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 116.7531, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 113.2464, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 143.8081, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 112.1331, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 123.1532, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 129.2489, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 105.6805, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 111.5076, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 141.8370, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 111.2129, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 126.4025, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 125.1243, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 129.7616, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 144.3465, \n",
      "TRAIN Epoch 01/20, NLL 127.6532, PPL 424.4053\n",
      "VALID Epoch 01/20, NLL 116.3004, PPL 261.9131\n",
      "TEST Epoch 01/20, NLL 115.7959, PPL 253.6506\n",
      "Model saved at model/E01.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 136.3772, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 129.3955, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 139.7018, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 114.9913, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 110.2778, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 131.0750, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 114.9439, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 123.9637, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 109.7341, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 129.1765, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 99.3877, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 121.1626, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 104.2103, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 121.8653, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 128.9036, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 122.5662, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 113.7654, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 105.9192, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 106.6813, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 124.1064, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 107.2109, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 116.9070, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 124.3481, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 122.2833, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 125.7838, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 119.4587, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 136.1077, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 118.3935, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 136.6711, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 96.1124, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 128.2650, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 103.8228, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 95.4701, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 120.8360, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 107.6895, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 102.3679, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 115.8063, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 112.5728, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 132.1436, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 128.4802, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 111.2384, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 122.1211, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 129.0386, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 109.8702, \n",
      "TRAIN Epoch 02/20, NLL 122.1097, PPL 326.3369\n",
      "VALID Epoch 02/20, NLL 112.6944, PPL 220.3838\n",
      "TEST Epoch 02/20, NLL 112.4423, PPL 216.0759\n",
      "Model saved at model/E02.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 121.3656, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 118.0422, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 98.8459, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 108.6346, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 84.7299, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 142.4072, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 120.3534, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 111.4544, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 122.5749, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 106.8422, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 99.4802, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 115.1945, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 121.5688, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 108.3168, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 122.9245, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 107.6878, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 91.7603, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 132.9309, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 137.4798, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 131.8430, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 130.9245, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 110.6217, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 126.0772, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 152.2289, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 122.5165, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 104.3576, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 122.7896, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 93.8465, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 135.9862, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 122.3176, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 127.8579, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 108.6474, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 88.0243, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 103.5781, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 115.0227, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 98.3512, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 137.2779, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 117.8304, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 145.0566, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 112.4350, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 102.6567, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 120.7682, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 108.0077, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 86.6993, \n",
      "TRAIN Epoch 03/20, NLL 118.6411, PPL 276.8629\n",
      "VALID Epoch 03/20, NLL 110.6245, PPL 199.5910\n",
      "TEST Epoch 03/20, NLL 110.1496, PPL 193.6437\n",
      "Model saved at model/E03.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/2104, NLL-Loss 141.1685, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 101.1680, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 105.3169, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 104.3819, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 83.6900, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 113.3966, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 110.9214, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 127.3607, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 102.2555, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 124.4204, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 116.2626, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 132.6036, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 130.3017, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 104.3255, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 106.1581, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 99.7063, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 107.3756, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 107.6711, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 114.3303, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 114.1710, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 120.5488, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 135.2506, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 110.8913, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 125.4936, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 135.7793, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 119.3989, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 106.1360, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 112.9627, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 123.7362, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 134.4257, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 123.0829, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 89.8818, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 111.0222, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 126.6193, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 117.2946, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 118.0974, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 131.1132, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 120.5489, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 130.7584, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 119.1504, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 127.8630, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 122.3682, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 130.5424, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 188.1027, \n",
      "TRAIN Epoch 04/20, NLL 115.7895, PPL 241.8599\n",
      "VALID Epoch 04/20, NLL 109.0239, PPL 184.8678\n",
      "TEST Epoch 04/20, NLL 108.5283, PPL 179.2013\n",
      "Model saved at model/E04.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 103.4490, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 107.6474, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 84.7109, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 99.1167, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 116.4998, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 113.9770, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 125.9529, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 102.2723, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 112.0787, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 108.3722, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 95.6561, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 105.0245, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 102.6195, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 124.6905, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 126.5547, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 116.0753, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 102.9185, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 106.6018, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 127.4947, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 121.8230, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 121.9739, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 114.8592, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 102.7046, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 100.4206, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 114.8156, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 127.2230, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 106.1882, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 116.5544, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 142.9618, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 98.0409, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 111.9133, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 121.4731, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 121.4275, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 122.5369, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 106.5058, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 106.5271, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 109.5597, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 102.6583, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 94.3228, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 129.9539, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 109.6867, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 113.4154, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 149.1434, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 114.7644, \n",
      "TRAIN Epoch 05/20, NLL 113.5986, PPL 218.0029\n",
      "VALID Epoch 05/20, NLL 107.0206, PPL 167.9608\n",
      "TEST Epoch 05/20, NLL 106.4014, PPL 161.8755\n",
      "Model saved at model/E05.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 140.0763, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 96.5805, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 133.8514, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 109.1840, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 89.0853, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 98.7632, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 117.7695, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 112.1790, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 94.4292, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 116.8878, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 94.7625, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 107.6218, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 95.4644, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 91.5859, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 83.3296, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 109.6105, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 87.4685, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 125.3768, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 102.5951, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 114.6053, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 107.7942, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 99.8842, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 100.6688, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 108.7541, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 135.3909, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 130.4897, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 109.1322, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 98.7614, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 96.3971, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 102.5763, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 129.1223, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 102.3861, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 122.2269, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 114.0217, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 113.0090, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 116.5005, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 109.9786, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 105.9683, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 98.2867, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 104.5961, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 98.8899, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 111.3853, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 95.0174, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 76.6709, \n",
      "TRAIN Epoch 06/20, NLL 111.7788, PPL 199.9875\n",
      "VALID Epoch 06/20, NLL 106.8699, PPL 166.7535\n",
      "TEST Epoch 06/20, NLL 106.2063, PPL 160.3730\n",
      "Model saved at model/E06.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 98.6330, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 88.5394, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 102.2613, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 107.3563, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 119.3813, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 123.0537, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 103.6950, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 100.9914, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 117.2231, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 111.0102, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 127.5318, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 123.8963, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 128.4593, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 107.9789, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 76.3003, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 116.5366, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 138.7462, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 115.6078, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 131.2834, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 98.0378, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 110.0701, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 102.4953, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 124.7945, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 97.6806, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 103.2310, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 115.0257, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 128.8620, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 111.8005, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 120.7106, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 94.7131, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 110.4492, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 133.1038, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 113.0527, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 129.0261, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 101.6116, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 105.8581, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 123.0583, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 105.5761, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 105.7141, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 135.8115, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 99.1392, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 109.0958, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 121.7988, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 100.9204, \n",
      "TRAIN Epoch 07/20, NLL 110.0414, PPL 184.1776\n",
      "VALID Epoch 07/20, NLL 105.6032, PPL 156.9415\n",
      "TEST Epoch 07/20, NLL 105.0379, PPL 151.6600\n",
      "Model saved at model/E07.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/2104, NLL-Loss 93.5490, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 121.8998, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 102.9167, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 136.4403, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 104.3290, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 114.3716, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 117.4881, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 121.2460, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 122.7627, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 126.3535, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 93.2873, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 104.9869, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 126.8158, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 105.4971, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 109.1281, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 122.5322, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 135.8782, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 99.9621, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 93.3773, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 128.5780, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 113.0051, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 112.6229, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 121.7864, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 129.6141, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 109.8661, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 109.9479, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 114.8338, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 99.7108, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 104.9131, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 100.2612, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 128.3043, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 109.7387, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 93.3746, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 96.6661, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 91.6312, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 77.5116, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 121.8284, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 120.2632, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 101.4478, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 107.6102, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 100.5842, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 109.7799, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 91.8550, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 124.4031, \n",
      "TRAIN Epoch 08/20, NLL 108.6122, PPL 172.1143\n",
      "VALID Epoch 08/20, NLL 105.6199, PPL 157.0664\n",
      "TEST Epoch 08/20, NLL 105.1491, PPL 152.4682\n",
      "Model saved at model/E08.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 126.5474, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 124.5041, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 102.7477, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 92.2540, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 90.1978, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 111.0538, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 116.3873, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 115.1891, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 99.4392, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 95.1785, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 100.1686, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 97.7411, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 101.0895, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 103.7033, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 117.3439, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 91.8543, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 94.2673, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 127.0178, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 97.8805, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 114.9651, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 121.1770, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 103.9191, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 74.1610, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 103.7739, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 99.4239, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 90.7417, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 102.8355, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 93.3539, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 105.0737, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 102.3203, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 105.7989, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 106.1646, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 91.7787, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 108.2164, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 99.9494, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 110.6439, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 106.9619, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 118.2413, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 104.4411, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 89.3519, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 112.6604, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 104.9192, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 99.4953, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 113.1095, \n",
      "TRAIN Epoch 09/20, NLL 107.2253, PPL 161.1639\n",
      "VALID Epoch 09/20, NLL 105.3553, PPL 155.0897\n",
      "TEST Epoch 09/20, NLL 104.8864, PPL 150.5658\n",
      "Model saved at model/E09.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 105.2964, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 117.7328, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 108.0125, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 117.4231, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 133.4244, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 113.7632, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 117.4226, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 107.5638, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 100.5433, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 96.7607, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 123.0471, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 107.5057, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 105.0197, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 105.4730, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 108.7273, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 96.1755, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 111.8719, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 87.1334, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 111.0236, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 108.6275, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 88.9490, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 123.4292, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 117.5917, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 91.6442, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 120.9150, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 109.1764, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 131.2896, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 101.3168, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 114.8209, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 91.7962, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 95.2875, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 108.3696, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 109.4357, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 114.0566, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 108.6838, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 114.6028, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 104.1685, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 138.0097, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 116.3698, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 130.0626, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 111.2990, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 93.0878, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 135.3382, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 114.4823, \n",
      "TRAIN Epoch 10/20, NLL 106.1187, PPL 152.9281\n",
      "VALID Epoch 10/20, NLL 104.5460, PPL 149.1957\n",
      "TEST Epoch 10/20, NLL 104.1983, PPL 145.6928\n",
      "Model saved at model/E10.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 114.6836, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 93.8544, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 119.1336, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 102.9255, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 116.9280, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 107.6734, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 112.0256, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 97.9593, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 103.4942, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 99.3919, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 86.1681, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 99.8065, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 130.5309, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 117.7246, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 105.7653, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 92.2040, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 115.7575, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 89.8536, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 78.8350, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 108.8722, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 115.7025, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 91.2906, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 102.4310, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 125.1630, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 104.1238, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 112.2279, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 140.5185, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 112.0094, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 112.9354, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 91.3219, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 106.2734, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 106.4189, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 101.6727, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 99.6221, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 102.7894, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 116.6228, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 118.3090, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 101.0566, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 111.5855, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 100.1009, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 120.9061, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 107.7615, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 103.5960, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 133.1867, \n",
      "TRAIN Epoch 11/20, NLL 104.8689, PPL 144.1318\n",
      "VALID Epoch 11/20, NLL 104.5595, PPL 149.2915\n",
      "TEST Epoch 11/20, NLL 104.1123, PPL 145.0954\n",
      "Model saved at model/E11.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 102.2738, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0050/2104, NLL-Loss 109.2526, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 111.0066, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 105.9933, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 94.1797, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 118.8873, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 128.0262, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 97.7539, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 112.0490, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 100.4920, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 107.0889, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 105.0523, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 115.5734, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 92.6262, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 100.9333, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 108.5869, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 85.3847, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 114.2403, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 97.5944, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 85.3093, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 92.8106, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 99.7781, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 109.9166, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 88.0113, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 136.6518, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 94.1181, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 109.4510, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 107.7356, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 116.9713, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 109.7704, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 121.7900, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 110.9278, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 105.4394, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 93.8014, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 116.5131, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 106.3662, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 114.4212, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 99.9574, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 106.3220, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 107.5832, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 99.0838, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 90.7970, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 96.5358, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 85.4458, \n",
      "TRAIN Epoch 12/20, NLL 103.8558, PPL 137.3739\n",
      "VALID Epoch 12/20, NLL 104.6805, PPL 150.1592\n",
      "TEST Epoch 12/20, NLL 103.7994, PPL 142.9409\n",
      "Model saved at model/E12.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 84.7041, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 112.5300, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 104.9479, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 93.9435, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 87.4142, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 125.5616, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 101.6376, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 87.8157, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 89.9188, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 91.2578, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 91.9376, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 98.3995, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 118.6049, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 82.4991, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 104.1314, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 96.4599, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 86.9728, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 128.7069, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 105.7820, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 94.4518, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 99.4830, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 104.4537, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 97.3591, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 101.2501, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 100.5256, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 92.3489, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 99.4300, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 89.1217, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 100.5696, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 104.9022, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 92.5128, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 99.2076, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 102.0838, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 113.8086, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 95.3232, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 89.3430, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 95.0726, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 108.7548, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 103.4688, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 111.1534, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 114.4193, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 76.8103, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 118.7659, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 97.0423, \n",
      "TRAIN Epoch 13/20, NLL 102.6937, PPL 130.0114\n",
      "VALID Epoch 13/20, NLL 104.2633, PPL 147.1894\n",
      "TEST Epoch 13/20, NLL 103.6771, PPL 142.1078\n",
      "Model saved at model/E13.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 85.2156, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 96.7704, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 85.9255, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 88.9551, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 96.9866, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 121.0200, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 120.5802, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 114.0695, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 85.0534, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 117.2093, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 86.6732, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 101.3395, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 95.5790, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 88.2177, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 119.1428, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 112.3475, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 96.2137, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 106.3043, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 93.5136, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 96.3321, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 134.0697, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 106.5785, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 115.9794, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 119.3651, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 88.1884, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 81.6201, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 113.2117, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 80.9696, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 110.5058, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 99.6561, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 110.4030, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 103.0094, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 102.8685, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 111.2761, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 91.7420, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 101.7693, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 109.5109, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 106.8726, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 72.8005, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 104.9703, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 90.8618, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 96.8158, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 96.3237, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 119.2469, \n",
      "TRAIN Epoch 14/20, NLL 101.7461, PPL 124.3011\n",
      "VALID Epoch 14/20, NLL 104.4678, PPL 148.6376\n",
      "TEST Epoch 14/20, NLL 103.8134, PPL 143.0370\n",
      "Model saved at model/E14.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 108.0788, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 91.4367, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 109.3579, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 96.8734, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 92.1771, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 99.2734, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 93.4583, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 89.6399, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 89.0139, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 82.6589, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 107.2612, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 95.7421, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 81.3998, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 71.8622, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 84.0384, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 111.6824, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 107.2997, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 97.6650, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 97.8922, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 104.5756, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 78.9025, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 97.4907, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 131.5293, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 102.0497, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 105.3181, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 103.1709, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 119.6711, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 109.2470, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 97.8005, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 112.6677, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 107.9757, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 104.9574, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 105.6078, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 116.6659, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 131.4307, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 96.1741, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 78.5159, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 109.5484, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 103.6268, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 91.4864, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 104.0909, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 110.5099, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 92.7570, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 103.0183, \n",
      "TRAIN Epoch 15/20, NLL 100.8576, PPL 119.1751\n",
      "VALID Epoch 15/20, NLL 104.5029, PPL 148.8882\n",
      "TEST Epoch 15/20, NLL 103.8489, PPL 143.2800\n",
      "Model saved at model/E15.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 130.5772, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 88.9220, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0100/2104, NLL-Loss 97.3695, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 82.9301, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 89.3774, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 111.2700, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 95.8578, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 101.8761, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 100.5721, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 106.8291, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 110.6896, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 102.1321, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 86.3093, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 115.7534, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 93.5727, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 102.9182, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 107.0337, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 120.5082, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 87.2144, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 76.7290, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 95.4930, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 96.4280, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 86.7851, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 85.0187, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 96.0527, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 90.9931, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 118.3116, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 81.1328, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 107.3759, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 102.7395, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 103.4894, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 113.2060, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 94.8788, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 105.8122, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 110.1372, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 122.5649, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 98.2804, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 79.3275, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 104.6796, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 96.7328, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 103.4946, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 109.7817, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 82.6758, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 139.5096, \n",
      "TRAIN Epoch 16/20, NLL 99.9406, PPL 114.1060\n",
      "VALID Epoch 16/20, NLL 105.1661, PPL 153.6908\n",
      "TEST Epoch 16/20, NLL 104.8441, PPL 150.2614\n",
      "Model saved at model/E16.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 83.7574, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 97.0118, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 131.1277, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 120.1854, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 80.3535, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 90.3979, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 102.7865, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 91.6975, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 75.8839, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 99.7863, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 113.9632, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 87.6961, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 93.0776, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 126.3685, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 100.0943, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 92.8026, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 111.6838, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 87.1251, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 107.0609, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 120.3360, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 119.0398, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 111.2597, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 67.0299, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 98.5759, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 90.5170, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 116.9151, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 104.2715, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 102.3837, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 102.8431, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 112.7299, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 84.5531, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 82.0427, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 101.6034, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 104.4744, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 109.8676, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 106.3070, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 87.4361, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 80.3452, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 85.9027, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 97.2836, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 98.1715, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 106.7152, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 99.5547, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 80.8954, \n",
      "TRAIN Epoch 17/20, NLL 99.0857, PPL 109.5746\n",
      "VALID Epoch 17/20, NLL 104.9532, PPL 152.1325\n",
      "TEST Epoch 17/20, NLL 104.2653, PPL 146.1603\n",
      "Model saved at model/E17.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 110.2033, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 98.5089, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 93.3461, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 114.8921, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 80.2275, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 99.9975, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 101.8351, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 85.2541, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 87.8040, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 95.6551, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 89.7578, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 131.8285, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 94.0303, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 83.4446, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 101.4536, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 98.0802, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 84.8986, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 88.1934, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 74.1015, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 100.8775, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 105.5486, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 102.3188, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 88.3627, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 107.4429, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 101.2302, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 107.4740, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 97.2579, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 86.1493, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 104.7103, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 119.5820, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 99.7489, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 100.8307, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 86.3513, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 91.5554, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 105.4948, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 78.1210, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 112.1325, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 107.4893, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 89.3656, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 91.0208, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 109.4874, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 83.4720, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 109.9813, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 96.1403, \n",
      "TRAIN Epoch 18/20, NLL 98.3576, PPL 105.8575\n",
      "VALID Epoch 18/20, NLL 104.8905, PPL 151.6763\n",
      "TEST Epoch 18/20, NLL 104.1554, PPL 145.3944\n",
      "Model saved at model/E18.pkl\n",
      "\n",
      "TRAIN Batch 0000/2104, NLL-Loss 78.4775, \n",
      "TRAIN Batch 0050/2104, NLL-Loss 95.9252, \n",
      "TRAIN Batch 0100/2104, NLL-Loss 91.2475, \n",
      "TRAIN Batch 0150/2104, NLL-Loss 86.8771, \n",
      "TRAIN Batch 0200/2104, NLL-Loss 85.7954, \n",
      "TRAIN Batch 0250/2104, NLL-Loss 124.8697, \n",
      "TRAIN Batch 0300/2104, NLL-Loss 71.8953, \n",
      "TRAIN Batch 0350/2104, NLL-Loss 96.7718, \n",
      "TRAIN Batch 0400/2104, NLL-Loss 83.6309, \n",
      "TRAIN Batch 0450/2104, NLL-Loss 76.5488, \n",
      "TRAIN Batch 0500/2104, NLL-Loss 104.8688, \n",
      "TRAIN Batch 0550/2104, NLL-Loss 80.4800, \n",
      "TRAIN Batch 0600/2104, NLL-Loss 94.0146, \n",
      "TRAIN Batch 0650/2104, NLL-Loss 111.5164, \n",
      "TRAIN Batch 0700/2104, NLL-Loss 103.2499, \n",
      "TRAIN Batch 0750/2104, NLL-Loss 99.5621, \n",
      "TRAIN Batch 0800/2104, NLL-Loss 95.9982, \n",
      "TRAIN Batch 0850/2104, NLL-Loss 108.8814, \n",
      "TRAIN Batch 0900/2104, NLL-Loss 109.5613, \n",
      "TRAIN Batch 0950/2104, NLL-Loss 107.7736, \n",
      "TRAIN Batch 1000/2104, NLL-Loss 109.5881, \n",
      "TRAIN Batch 1050/2104, NLL-Loss 88.7574, \n",
      "TRAIN Batch 1100/2104, NLL-Loss 125.8678, \n",
      "TRAIN Batch 1150/2104, NLL-Loss 87.5458, \n",
      "TRAIN Batch 1200/2104, NLL-Loss 99.0712, \n",
      "TRAIN Batch 1250/2104, NLL-Loss 89.9719, \n",
      "TRAIN Batch 1300/2104, NLL-Loss 80.9149, \n",
      "TRAIN Batch 1350/2104, NLL-Loss 99.4153, \n",
      "TRAIN Batch 1400/2104, NLL-Loss 98.5990, \n",
      "TRAIN Batch 1450/2104, NLL-Loss 107.5482, \n",
      "TRAIN Batch 1500/2104, NLL-Loss 71.0803, \n",
      "TRAIN Batch 1550/2104, NLL-Loss 92.7405, \n",
      "TRAIN Batch 1600/2104, NLL-Loss 106.6250, \n",
      "TRAIN Batch 1650/2104, NLL-Loss 98.6803, \n",
      "TRAIN Batch 1700/2104, NLL-Loss 84.8470, \n",
      "TRAIN Batch 1750/2104, NLL-Loss 84.3888, \n",
      "TRAIN Batch 1800/2104, NLL-Loss 103.1779, \n",
      "TRAIN Batch 1850/2104, NLL-Loss 100.2962, \n",
      "TRAIN Batch 1900/2104, NLL-Loss 89.6015, \n",
      "TRAIN Batch 1950/2104, NLL-Loss 101.3986, \n",
      "TRAIN Batch 2000/2104, NLL-Loss 119.5211, \n",
      "TRAIN Batch 2050/2104, NLL-Loss 102.1828, \n",
      "TRAIN Batch 2100/2104, NLL-Loss 100.7184, \n",
      "TRAIN Batch 2103/2104, NLL-Loss 92.4940, \n",
      "TRAIN Epoch 19/20, NLL 97.4301, PPL 101.3048\n",
      "VALID Epoch 19/20, NLL 105.2653, PPL 154.4230\n",
      "TEST Epoch 19/20, NLL 104.6545, PPL 148.9053\n",
      "Model saved at model/E19.pkl\n",
      "\n",
      "Total cost time 00 hr 17 min 35 sec\n"
     ]
    }
   ],
   "source": [
    "# training interface\n",
    "step = 0\n",
    "tracker = {'NLL': []}\n",
    "start_time = time.time()\n",
    "for ep in range(epoch):\n",
    "    # learning rate decay\n",
    "    if (ep % 2 == 0) and (learning_rate>0.1):\n",
    "        learning_rate = learning_rate * 1 #0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    for split in splits:\n",
    "        dataloader = dataloaders[split]\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        totals = {'NLL': 0., 'words': 0}\n",
    "\n",
    "        for itr, (_, dec_inputs, targets, lengths) in enumerate(dataloader):\n",
    "            bsize = dec_inputs.size(0)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # forward\n",
    "            logp, NLL_loss = model(dec_inputs, lengths, targets) #, lengths\n",
    "\n",
    "            # calculate loss\n",
    "            #NLL_loss = NLL(logp, targets, lengths + 1)\n",
    "            loss = NLL_loss / bsize\n",
    "\n",
    "            # cumulate\n",
    "            totals['NLL'] += NLL_loss.item()\n",
    "            totals['words'] += torch.sum(lengths).item()\n",
    "\n",
    "            # backward and optimize\n",
    "            if split == 'train':\n",
    "                step += 1\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 0.25) #5\n",
    "                optimizer.step()\n",
    "\n",
    "                # track\n",
    "                tracker['NLL'].append(loss.item())\n",
    "\n",
    "                # print statistics\n",
    "                if itr % print_every == 0 or itr + 1 == len(dataloader):\n",
    "                    print(\"%s Batch %04d/%04d, NLL-Loss %.4f, \"\n",
    "                          % (split.upper(), itr, len(dataloader),\n",
    "                             tracker['NLL'][-1]))\n",
    "\n",
    "        samples = len(datasets[split])\n",
    "        print(\"%s Epoch %02d/%02d, NLL %.4f, PPL %.4f\"\n",
    "              % (split.upper(), ep, epoch, totals['NLL'] / samples,\n",
    "                 math.exp(totals['NLL'] / totals['words'])))\n",
    "\n",
    "    # save checkpoint\n",
    "    checkpoint_path = os.path.join(save_path, \"E%02d.pkl\" % ep)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Model saved at %s\\n\" % checkpoint_path)\n",
    "end_time = time.time()\n",
    "print('Total cost time',\n",
    "      time.strftime(\"%H hr %M min %S sec\", time.gmtime(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 5221152\n"
     ]
    }
   ],
   "source": [
    "print('# of parameters:', sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning results\n",
    "sio.savemat(\"results.mat\", tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-virtual",
   "language": "python",
   "name": "python3-virtual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
